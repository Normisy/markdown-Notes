
Apache Flink是一个框架和分布式处理引擎，可以用于有界数据流（批处理）和无界数据流（流处理）两种场合
# C1. Flink的基础概念

## 1.1 批次处理与流式处理

传统批次处理方法的特点在于：
- 持续收取数据
- 以时间作为划分数个批次档案的依据
- 周期性地执行批次运算
在这种数据处理方式中，数据是持续进行输入的，具有time series的特性，会使用队列的形式，以时间作为划分，将所收集到的数据划分为一个个的批次档案，再利用批次运算引擎，例如spark等，对其进行处理

然而，如果需要计算处理的内容跨越了所定义的时间划分，传统批次处理的方法是将其保存在一个中间运算结果中，然后将这个结果带入下个批次运算中
并且，若接收数据的顺序和真实事件发生的顺序是颠倒的话，例如先发生A,再发生B事件，但接受数据的顺序是先B后A，这种情况下的计算就更加复杂了

### 1.1.1 累积状态和时间完整性验证

一种理想的处理方式是有一个一直在执行的进程负责监听某个队列中所收到的数据，它会在长时间的运行中维护一个累计状态，这个累积状态代表着过去历史中接受过的所有的事件
例如，这个进程持续利用所收到的数据来进行一个机器学习模型的训练，模型每个时刻所训练出来的参数，就是一个累积状态
当然，状态可以是各种各样的东西，重点在于：**累积状态会影响最后每个所接收到的数据产生的输出**

因此框架必须提供累积和维护状态的能力，并且**引擎必须能够判断它是否已经接收到所有它计算结果所需要的数据了**
在批处理中，按照时间段划分的方法并不能检验这段时间内的资料是否已经完整了，因为时间划分是我们人为进行的

总结，我们理想的流处理引擎应该是这样的：
![[Pasted image 20250317192303.png]]
### 1.1.2 流式处理及其分布式

流式处理就是对于一段无穷的数据输入流，我们编写一段代码，这段代码会一笔一笔地取得输出，进行处理后一笔一笔地进行输出

将其放在分布式系统中进行，输入数据有很多个使用者，每个使用者都有自己的id，每个使用者必须访问相同的实例。因此，我们会对输入流进行分区，将它们按照某个键值key进行分离，每个计算实例对应一个key，符合这个key的数据就由它处理
![[Pasted image 20250317194335.png]]


在每个计算节点的处理过程中，代码会根据接收到的数据更新局部变量或数据结构。这是有状态流处理的关键所在，即**每个数据项的处理不仅仅依赖当前输入的数据，还可能依赖先前的计算结果**：
![[Pasted image 20250317194708.png]]
当我们对一个数据流按照键值进行分区之后，状态也应该随之被一起分区
并且，对于一个数据量很大的情况，相同key的数据量可能超过了该计算节点的负荷，此时应该有一个状态后端进行负载均衡等

## 1.2 有状态流式处理的挑战及解决方法

### 1.2.1 状态容错

在分布式状态流式处理中，如果我们的一个计算节点发生错误（例如挂了），应该怎么确保它拥有状态容错？
状态的**精确一次的容错保证**：程序计算状态时，我希望每一笔输入的事件，反映到状态的更改有且仅有一次（也就是**一笔输入只导致一次状态更改**），如果会导致多次更改，这样的输出状态结果就是不可靠的

#### 简单场景下的精确一次容错方法
考虑单一计算节点进程接受无穷输入流的最简单场景，这个进程维护一个累积状态，每获得一笔输入，对状态进行一次更新
对这个情况下的精确一次容错，就是每更改一次状态后，对其进行一次快照：这个快照包含了当前接受的输入笔数（在队列中的位置），以及当时的状态是多少。

那么问题就在于：
- 如何在分散式场景下，替多个具有本地状态的运算节点产生一个全局一致的快照
- 如何在不中断计算的前提下进行快照的生成与记录？

全局快照的最简单的实现是：当处理完一笔输入后，这笔输入分区后进入各个节点，更改每个运算节点的状态，此时我们将每个节点的状态加上这笔输入的位置结合起来，成为一个全局快照

但是，这个方法存在副作用：一笔数据流经多个节点时，对于前面的节点，它们很早就完成了计算，本来可以接着处理第二笔数据的，但是为了生成包含现在状态的全局快照，前面已经完成更更新的节点不得不停下来，等待全部节点更新完成后生成快照

因此，这就引入了我们的第二个问题：如何在不中断计算的前提下生成快照？
flink提供了一个**检查点checkpoint机制：在每个计算节点本地的状态后端中，维护一些检查点，当到达检查点的时候，状态后端会将该计算节点的状态传输到一个共享内存中**。如果某个计算节点挂掉了，那么状态后端恢复状态到上一个检查点，然后把数据流（例如我们使用的kafka数据源）恢复到相应位置，重新进行计算

#### 分布式快照方法与检查点
FLINK会在输入的数据流中持续安插检查点屏障checkpoint barrier：
![[Pasted image 20250317222427.png]]
由任务管理器触发checkpoint，触发后启用检查点屏障，此时认为**当前屏障以前到上一个屏障之间的所有数据**由当前这个检查点记录
此时需要完成的任务是填好一个类似如下的表格：![[Pasted image 20250317222719.png]]
- 首先，数据源需要保存自己当时的状态，例如目前这段数据在kafka分区中的位置，这段状态对应的数据用于恢复到上一检查点状态后进行重计算
- 后面则是每一层的计算节点了，计算节点的层次结构按照数据的流动进行划分，一层可能有多个计算节点：**源数据流进行第一次划分，交给第一层计算节点进行计算；第一层计算节点的输出作为第二层计算节点的输入......以此类推**，就如下图所示：
![[Pasted image 20250317223443.png]]

![[Pasted image 20250317223529.png]]
（红色的点代表对应于检查点屏障n的数据）

这样生成的完整的表格就可以用于状态的恢复和重计算了，注意：**只有完整地经过了源和所有计算节点并且全部上传完快照的CHECKPOINT才能使用**

### 1.2.2 状态维护

状态维护的含义就是我们写一段代码，这段代码用于本地维护一些我们感兴趣的状态值，如果这些状态值的负载对于当前节点过大了，那么需要由状态后端进行调度
下面展示的代码就是在一个计算节点上进行本地状态后端的注册：
![[Pasted image 20250317224209.png]]
flink提供许多api进行状态的注册，状态后端的类型有很多种，但是无论如何，在完成注册之后，所有状态的读取和更新都是通过状态后端进行访问的
（例如，上图的对象`state`就是一个注册了的状态后端，下面对状态的读取`val p = state.value()`和写入更新`state.update()`都是经过这个状态后端进行的）

flink的状态后端主要分为两种类型：（详细介绍在后面的课程讨论）
- JVM heap状态后端：适合用于比较小的状态量。它在每一次计算节点需要访问读取状态值的时候，进行的是Java 对象的 read / write，实际上并没有太大的代价，**因为只涉及到内存中java对象的读写，在memory中进行**
  然而当我们产生检查点，要将每个运算节点的本地状态放到共享的分散式快照时，就需要进行序列化（因为之前的过程都是在内存中的，没有经过持久化写入磁盘），在内存中进行序列化，存储到**分布式文件系统DFS中**
- RocksDB状态后端：是一种**基于磁盘的状态后端**：将状态存储在本地磁盘上
  在运行时本地状态后端中，使用者请求读取状态时都需要直接经过磁盘，状态可以维护在磁盘中，但代价在于每次读取状态的时候都需要经过一个序列化和反序列化的过程
  该类型状态后端下，我们产生快照就很简单，因为状态已经是序列化好的了，直接传输到共享的DFS中即可
  适用于状态量比较大，难以存储在内存中的情况

### 1.2.3 Event-time处理

在流式处理引擎出现之前，最常见的处理方式是processing-time，是指事件在被处理时**所处的机器的本地时间**，即事件被流处理系统接收到并处理的时间（流处理系统的当前时刻）
例如，对于一个每小时进行结算的时间窗口处理，processing-time的处理方式是对这段时间接收到的资料进行计算，但是现实中我们的需求实际上是想知道现实生活中这段时间真正发生的资料的计算结果，这需要参考**事件本身携带的时间戳**，而不是数据被接收到的时间

event-time指的就是这个事件最开始发生时所携带的时间戳，通常由数据的生产者所指定
![[Pasted image 20250318164042.png]]

这就导出了我们上面对于一个流式处理引擎的需求：怎么确认运算所需的资料已经全部收集完毕，何时才能输出运算结果？
要回答这个问题，必须要让处理引擎具有event-time的认知，在flink中，这个功能的实现基于一个名为watermarks的**特殊事件**

#### Watermarks水位线
**watermarks是flink中的一个特殊事件**，一个带有**时间戳t**的watermark会让计算节点**判定不会再收到任何时间戳<t的事件了**
产生水位线的时候，例如对于三点到四点时间段的数据接收，生产者往往会向四点以后推迟一定时间，确保不会再出现这段时间内的数据后，生成一个水位线事件进行标记
（如果运算节点还是收到了早于水位线时间戳的数据，flink提供了side-output的功能，将这些事件输出到一个特定位置，可以被我们从运算节点中截取出来并定义使用方式或是丢弃）

![[Pasted image 20250318173924.png]]
### 1.2.4 状态保存与迁移

流式处理应用持续在运行，对于运维上会有以下几个重要考虑：
- 如果当前的应用的某个逻辑需要更改，或是需要进行某个bug的修复，修改完以后，对于之前累积的状态，我们需要将其迁移到新的执行逻辑中
- 如果我们希望修改应用的并行化程度，如何将之前的状态迁移，并且将数据重分区到新的并行计算节点中
- 希望将flink框架的版本升级到最新版本，应该如何进行数据的迁移？

实际上flink的检查点机制很适合用来处理这样的问题，对于**手动产生的检查点，flink称其为保存点savepoint**
保存点记录着流式应用中所有运算节点的状态，在执行停止之前，产生一个保存点，在完成更改后，恢复执行
因为更改过程需要一定时间，这个过程中kafka等数据平台一直持续在进行资料的搜集，保存点记录了暂停时数据在kafka中的位置，重新启动后数据的处理必须要赶上最新的搜集
这个过程如果使用processing-time，是通过将这段时间的所有数据都放在一个单一的时间窗口之内，是不合适的；但因为采取了event-time的处理方式，状态迁移时能够很好地保持数据处理的一致性

## 1.3 small talk

### 1.3.1 输入数据异常时，水位线机制和运算节点的行为？

假设一个计算节点接受一个或多个上游节点的输出作为输入，如果存在输入异常的情况，例如一直没有输入，此时因为水位线是按照输入数据的情况进行生成，因此此时并不会产生水位线，此外，运算节点如果在多个上游节点的输出中发现任何一个节点没有输出水位线，此时它的计算会暂停

### 1.3.2 多流水位线处理内部的实现方式

一个运算节点接受多个输入，输入都会产生水位线，此时该运算节点的event-time取得是所有这些水位线中（时间）最小的那个

### 1.3.3 水位线是如何标记的？

水位线是一种特殊的事件，它的产生也基于我们处理的事件，因此我们需要对自己处理的事件的特性有一定的认知
在flink中，我们首先定义一个一直在输出资料的数据源，相当于是一个节点，在这个运算节点后面会调用一个API：`assignTimestampsAndWatermarks()`，这个API将时间戳分配器和水位线生成策略与数据流进行绑定，flink提供的类或多种API提供了不同的策略

# C2. flink的实现介绍

## 2.1. 运行flink应用

### 2.1.1 flink的数据流处理流程

运行flink应用之前，有必要了解flijnk运行时的各个组件，因为这与应用的配置问题相关
#### 数据流
flink的数据流表示图如下：
![[Pasted image 20250323131531.png]]
首先，数据流必定拥有一个数据源`Source`，这个数据源一开始会经过一次`map()`操作，它的作用是对输入流中的每个元素执行指定的转换逻辑，生成一个新的流
这是因为数据流往往来源于不同的系统，数据格式可能不一致、数据需要清洗，此时需要对数据进行格式转换以及复杂的差值等处理，因此需要这么一步操作

- `map()`是一个单一元素的转换操作，对输入流中的元素应用我们自己指定的函数，输出一个新的元素流，这个操作只依赖于输入的单个元素，每条数据的处理是独立的，因此能够在多个节点中并行执行（上图中每个输入源都有`map()`）

- `keyBy()`操作根据某个字段或某种方法对数据进行分区，例如根据某个键值`key`进行分组处理，同组数据分配到同一个下游子任务中，从而可以在窗口计算和状态管理中实现逻辑的隔离
	它接受一个函数`KeySelector<T, K>`，其中`T`是输入流中的元素，`K`是根据输入元素生成的分组键值，分组操作是逻辑上的，不改变数据结构
	往往采用哈希分区策略：对每个元素调用`KeySelector`函数提取key,然后根据设定的并行度，利用哈希函数将key随机打散，使用其`哈希值 % 并行度`来决定数据发往哪个子任务


- `window()`操作就是时间窗口操作，它将无限数据流划分为有限块的方式，类似于通过时间或计数划分的批处理方式，窗口有着的不同类型，基于时间的窗口：
	-  `Tumbling Window`：滚动窗口，不重叠。
	- `Sliding Window`：滑动窗口，窗口之间可以重叠。
	- `Session Window`：会话窗口，根据数据间的间隔时间动态分割。

- `apply()`操作是对`window`操作中窗口里收集的数据进行自定义处理的接口，它可以访问整个窗口内的元素以及窗口的元数据，例如开始和结束时间等
	在这个接口中，用户可以实现复杂的逻辑，比如排序，查找，插值等等，获得的输出会通过`Collector`发送到下游算子或Sink中

### 2.1.2 进程级别的执行

将处理流程具体落实到进程之间的执行，不同key值的数据会被分割到不同的task中，task是flink中资源调度最小的单位
![[Pasted image 20250323214748.png]]
flink program是用于提交flink作业的命令行工具：
- 首先将用户代码进行优化和建图：程序的计算逻辑会被表示为数据流图，这是一个表示作业计算逻辑的图形结构
- 然后图生成器将数据流图传递给客户端，客户端传递给`jobManager`进程
可以看到整个flink程序运行时会出现两类进程：
- `TaskManager`：负责**执行具体的任务**，在每个进程中都会有多个任务槽，它们是资源管理的最小单位，每个槽运行一个任务。`taskmanager`中还具有内存管理器、网络管理器、IO管理器等组件，用于不同进程之间的通信，通过网络管理器来在不同的`TaskManager`之间传递数据
- `JobManager`：负责作业调度和资源分配，通过消息传递机制协调组建之间的通信（Actor System负责状态、检查点等的维护）


# Cex. Flink CDC
# 1. 介绍

CDC技术，即变更数据捕获技术，是一种用于实时捕获数据库数据变更的解决方案，用于在数据变更时进行追踪，将变更同步到其他系统，保证数据一致性

目前cdc的最好方式是基于日志的CDC，这种方法持续监听和读取数据库事务日志，是一种流式处理的模式，能保障数据的实时性增量更新

下面是一些开源的CDC方案的对比
![[Pasted image 20250325200730.png]]


传统数据入仓架构：基于Hadoop生态，其中的HDFS和Hive分别是分布式文件系统和数据仓库，提供了文件的分布式存储功能，以及类SQL的查询功能
![[Pasted image 20250324204835.png]]
通过多条链路定时进行全量同步、增量同步和合并操作，再存储到HIVE中用于查询
这种方式的链路割裂性太强，不同链路的定时操作带来的数据延迟较大，所用组件太多也导致了性能和管理上的问题

## 2. 基于FLINK CDC的海量数据实时同步和转换

## 2.1 全量+增量无锁算法
FLINK CDC实现了一个支持并发读取的增量快照读取的无锁算法，让我们仔细研究一下其优越性的来源：
核心流程：
- 数据分块：
	- 根据表的主键，通过`JobManager`中的`SourceCoordinator`将全量数据划分为多个独立的chunk，每个chunk的范围通过在主键的范围内以固定步长划分得到，记录每个chunk的元数据
	- 分割好chunk之后，`TaskManager`中的`SourceReader`负责实际并行读取chunk。此时`SplitEnumerator`会监听多个`SourceReader`，将chunk分配给空闲的`SourceReader`，这个过程会随着全量阶段和增量阶段的不同而进行变化和调度
- 读取chunk时采用无锁算法优化：
	- `binlog`：是二进制日志，用于记录数据库的所有修改操作，并以事务作为单位存储
	- 低水位线：获取当前binlog的偏移量（当前二进制日志文件中最后一个已记录事件的结束位置，以字节为单位），作为低水位线以记录此时数据库的状态
	- 全量读取：读取chunk的全量数据，期间允许上游数据的更改
	- 高水位线：全量读取之后再获取一次binlog的偏移量，标记此后的增量变更
	- 变更合并：读取高低水位线之间的binlog变更事件，将其与全量数据合并
- 全量增量同步的无缝切换：
	- SourceReader 支持 Chunk 粒度的 Checkpoint，将当前处理进度（如已读取的 Chunk 偏移量）上报给 SplitEnumerator，实现断点续传
	- 所有 Chunk 处理完成后，根据高水位线切换到纯增量模式，持续捕获并处理 binlog 事件

因此，基于flink cdc的入湖架构通过这样一个流程，进行全增量实时同步的方式，传递给数据湖

数据乎可以存储大量的原始数据，兼容不同来源和不同格式的数据

Apache Hudi（Hadoop Upserts, Deletes and Incrementals）是一种面向数据湖的开源存储框架，旨在高效管理大规模数据的更新、删除和增量处理
![[Pasted image 20250324223553.png]]

## 2.2 无锁切换
Apache flink CDC提供的全增量一体化框架中，提供了全增量无锁切换的机制：

![[Pasted image 20250325170921.png]]
- 在全量阶段，CDC从数据源中读取所有数据，此时flink扫描整个数据源，将所有数据分批次加载进来，这个任务是并行执行的
- 增量阶段主要关注数据源的实时变化，通过基于日志的CDC技术监控数据库中数据的变动，并将增量数据进行处理和传输
- 全量阶段实际上也允许之前已经读取数据的修改，事实上就是上面2.1所说的全增量算法
- 在全量阶段结束后，会设置一个标志位，并且获取checkpoint，系统根据标志位判断当前阶段是否进入增量，进入增量后通过监听二进制日志的方式，捕获数据的变化情况，把高低水位线之间的所有变化同步到已有数据中

## 2.3 面向存储友好的设计

在flink cdc的设计中，面对下游接收数据的apache hudi等数据湖组件，也设计了一些面向存储友好的写入方式
这个设计就是将原数据进行切片为chunk,因为hudi的writer是将数据全部读取到内存中，然后写入存储中。若hudi读取的数据过大，很容易造成内存不够的问题，因此写入时会进行切片（切片的大小允许用户自行选择）

## 2.4 Transformation

flink的数据清洗、聚合、打宽等transformation的操作

![[Pasted image 20250325194923.png]]


# C3. Data Stream API介绍

## 1. 分布式流处理

分布式流处理的逻辑模型使用DAG图的形式来描述，但具体的物理模型可能会更复杂一些：
![[Pasted image 20250406221345.png]]
首先，每一个算子可能会有多个不同的实例，并且上游的实例节点可能会同时与下游的一个或多个实例节点进行传输
另外，系统会根据自己执行的一些优化，将多个实例节点分布在不同的计算节点之上（黄色和蓝色椭圆），在单机上进行的数据传输在本地执行，而涉及多个计算节点之间的数据传输，就需要经过网络

## 2. 流处理API的演变

Apache Storm：
```java
TopologyBuilder builder = new TopologyBuilder();

builder.setSpout("spout", new RandomSentenceSpout(), 5);
builder.setBolt("split", new SplitSentence(), 8).shuffleGrouping("spout");
builder.setBolt("count", new WordCount(), 12).fieldsGrouping("split", new Fields("word"));
```
strom作为比较早期的流处理引擎，它的DAG图建立模式很简单，首先声明了一个建图器，通过这个对象进行DAG图中节点的设置，以及节点之间的连接方式指派
这主要是一种**面向操作的底层结构**，但实际上建图器可能会对流程进行一定优化，最后的图未必和最开始定义的相同

而在flink中，API会更高层一些，它是**面向数据的**，并不关心具体的底层操作

## 3. Flink DataStream程序结构


下面是示例程序`WordCount`的主程序代码展示：
```java
//设置运行环境
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

//配置数据源，读取数据
DataStream<String> text = env.readTextFile("input");

//进行一系列数据的转换
DataStream<Tuple2<String, Integer>> counts = text.flatMap(new Tokenizer()).keyBy(0).sum(1);

```
- 设置运行环境：目前的很多分布式计算框架都被设计为在实际的操作之前都需要获取一个环境（上下文）
- 配置数据源：一个数据处理程序的起点通常是通过一个数据源接入数据，第二段代码就是读取数据源的一个API，它会通过特定的API从给定类型数据源接取数据
- 数据转换：输入的数据流可以想象成是以一个集合的形式出现的，那么数据转换可以视作是在这个集合上进行的操作，这里包含了：
	- `flatMap(new xxx)`：这个操作接受一个用户定义的函数（指针），对集合进行转换操作。这里的操作`Tokenizer`是句子拆词划分
	- `keyBy(0)`：根据某种规则进行键的分配，此处参数0代表按照输入值本身进行分组（相同输入值分到同一组）
	- `sum(1)`：求和操作，