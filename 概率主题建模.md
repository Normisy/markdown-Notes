# 1. 建模过程
在文本和信息量很大的条件下，进行逐个文档的阅读来找到我们想要的信息是不现实的。我们日常的信息检索中，往往会将感兴趣的文档内容**放大为一些包含文档主题的关键词，通过这些主题的交集来寻找最符合的文档**；或是将文档的内容**缩小细分至某些我们对此都很感兴趣的主题，寻找同时包含这些主题的文档**。
主题建模算法就基于这样的直觉，根据给定文本的词汇进行统计，分析贯穿一个文档的主题、主题间的联系方式以及主题的时间演变。
主题建模算法不需要预先对原文档进行预处理，它直接根据文档的原始信息进行统计、分析。
## 1.1 介绍
潜在迪利克雷分配LDA是最简单的概率主题模型。
LDA将文档视为一个容纳了许多单词的单词表，而我们能够统计到的有关文档的信息是有关文档中词语的统计信息。
LDA背后的直觉是一个文档的生成基于多个主题的混合，这样的主题由一组文档中的**单词分布**构成，因为单词本身具有主题的倾向，例如“有机体”、"生命”等单词与主题”进化生物学“有着较强的关联。通过这种直觉，我们可以去掉文档中含有主题倾向很少的单词，对剩下的单词进行统计：
![[Pasted image 20240920092624.png]]

上图中，我们使用不同颜色标记出与不同主题相关性较强的单词，忽略掉主题倾向不明显的单词，这就是所谓单词的”**主题指派**“。
左侧主题统计框中统计了与不同主题相关的单词在文档中的占比，它们描述了**不同主题在文档中的分布**
右侧统计直方图则将不同主题的单词总占比进行统计，放在一张图中比较，这描述了**主题在文档中的占比**。

LDA是文档的统计模型，该模型猜测文档生成于一个假想的随机过程，并使用这样的生成过程来描述整个模型。

作为一个典型的贝叶斯分析模型，LDA将所有我们能够统计到的文档信息视为先验信息，我们的目的是通过这些先验计算出我们感兴趣的隐参数的后验分布。
首先对我们讨论的**主题分布**进行一个正式的解释：我们的算法没有接受任何关于主题和文档的信息，也就是说没有从一开始就被标记为“生物学”、“数据挖掘”等主题标签的文档。在LDA中，我们认为主题的分布是在最开始生成文档的时候就选择好的，一个主题被假设为一组单词的概率分布，在训练过程中根据文档中词汇的共现情况学习出主题的高概率单词并不断调整。因此主题可以根据文档中的主题高概率词汇的占比进行表征，同时这些占比的总和表明主题在整个文档的主题中的占比是多少。

## 1.2 文档生成过程的假设
LDA将一个文档的生成过程描述为如下的随机过程：
1. 首先随机选取一个主题分布
2. 对于文档中的每个单词（位置）：
	1. 随机从步骤1的分布中随机选取一个主题
	2. 得到主题后，该主题对应于一个单词分布，从单词分布中随机选取一个单词放在该位置上。
重复上述过程生成集合中的所有文档

从这个过程中可以看出：首先，我们得到的文本本身是一种表征，在文本背后的主题结构——每个文档的主题分布、主题对应的单词分布，以及反过来单词对应的主题指派，是我们感兴趣的隐藏结构
而且可以知道，在LDA中，一个文档由多个主题混合形成，可以由主题的一个多项分布表示。同理，一个主题又是由一个单词的多项分布来表示的，这就是LDA的显著特征。
我们的工作就是根据观察到的文本反推出隐含的主题结构是什么

文章以从《science》中统计的17000篇文章为例，使用假定有100个主题的主题建模算法绘制出如下的主题分布和不同主题的高概率词汇（部分）
![[Pasted image 20240920114843.png]]
我们得到的这些信息都是主题分布上的抽样，仅仅是一小部分。但是从不同期刊中得到的这些抽样主题的单词分布显然是会有所不同的，例如下面的在耶鲁法律期刊中统计得到的主题：
![[Pasted image 20240920115024.png]]
比起“基因”这样的科学名词，它更倾向于“税率”“政策”这样的政治经济学名词
因此这样的算法为大型档案的分类、管理提供了有效的思路。
## 1.3 正式的数学描述
LDA是概率建模领域的一个模型，我们将数据视为产生于包含隐变量的生成过程，该过程定义了我们可以观察到的变量和观察不到的隐变量的联合分布，我们使用这样的联合分布来计算出给定观察变量的条件下，隐变量的条件分布，也就是贝叶斯分析中的后验分布
LDA中，观察到的变量是文档的单词，隐变量是主题结构

使用符号语言对我们之前所说的变量进行描述：
- 文档的主题$\beta _{1:k}$，其中$\beta_k$是一个主题，即图1的
![[Pasted image 20240920125915.png]]


- 第d个文档的主题占比$\theta_d$，其中主题k在第d个文档中的主题占比为$\theta_{k,d}$  

- 第d个文档的主题分配$z_d$，第d个文档的第n个单词的主题分配为$z_{d,n}$  

- 我们在第d个文档中观察到k的单词为$w_d$，第d个文档的第n个单词为$w_{d,n}$

LDA的生成表示为：
$$ p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) = \prod_{i=1}^{K}p(\beta_i) \prod_{d=1}^{D}[p({\theta_d})(\prod_{n=1}^{N} p(z_{d,n}|\theta_{d})p(w_{d,n}|\beta_{1:K},z_{d,n}))] $$
公式指定了很多依赖关系：主题分配取决于每个文档的主题比例，观察到的单词又取决于所有的主题和主题分配。
该公式是根据”寻找主题分配$z_{d,n}$所指向的主题是什么，并且观察到的单词$w_{d,n}$在该主题中的概率占比是多少“来定义的

这些相关关系可以用这样的概率图形来表示
![[Pasted image 20240920131215.png]]

因此我们可以计算后验分布为：
$$ p(\beta_{1:K}, \theta_{1:D}, z_{1:D}|w_{1:D})=\frac{p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})}
$$
分子是所有变量的联合分布，也就是先验分布；分母是观测得到的单词的边缘分布，即在所有主题模型下都能看到被观测的语料库的概率
理论上来说，分子通过下面这样的计算：
在LDA的假设中可以看出，**一个文档中的主题的分配和一个主题中的单词的分配都是一种多项分布**。根据贝叶斯分析，我们知道**多项分布的共轭先验分布是迪利克雷分布**，因此我们选用迪利克雷分布去拟合先验分布可以得到分子；而分母”只需要“对所有可能的主题分布进行积分/求和即可

但是由于主题结构的数量过于庞大，而且维度可能很高，分母中边缘分布的计算极其复杂，无法对分母进行精确求解，只能通过算法来进行逼近。

主要使用的算法有吉布斯抽样算法、变分推理和变分EM算法等