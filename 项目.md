# 1. 轨迹流数据

## 1.1 概述
轨迹数据是具有时空特征的，通过**对一个或多个移动对象运动过程的采样**所形成的数据信息，一般包括**采样点位置信息、采样时间信息、速度等**，可分成人类活动轨迹，交通工具活动轨迹，动物活动轨迹和自然规律活动轨迹

轨迹数据具有量大、实时、多样的特点，主要特征有：
- 时空序列性：采样序列具有位置和时间信息，蕴含了时空动态性
- 异频采样性：活动轨迹具有随机性、时间差异较大的特征，因此轨迹的采样间隔差异显著，增加了轨迹数据分析的难度
- 数据质量差：连续性的运动轨迹被离散化地表示，受到采样精度、位置的不确定与预处理方式的影响，为数据分析带来一定困难

## 1.2 预处理
轨迹数据模型的原始轨迹数据具有很多冗余和噪声，需要通过数据清理、轨迹压缩、轨迹分段等预处理方式转换为**校准轨迹数据**，而**校准轨迹数据的存储、索引与查询是本项目的主题所在**，因此预处理部分简单涉及一下即可

### 1. 数据清洗
数据中的冗余点与噪音点：
- 停留点：移动对象在某一时间段驻留时间较长，对于这种数据，我们往往关心的是停留区域以及基于速度的行为特征
- 噪音点：由于软硬件而导致的错误采样，极大地影响了数据挖掘与分析的结果

#### a. 噪音滤波
- 中值或均值滤波：设单个噪音点测量值为$z_i$，利用真值和其前n-1个轨迹点的均值或中值进行真值的估计，中值滤波鲁棒性较强，但是对于多个连续噪音点的处理效果不佳

- 卡尔曼滤波：包含测量模型和动态模型，对于当前点的估计取决于先前的测量，包括了更多的状态向量，比如速度。但是动态模型是线性的，目标的运动必须符合预定义的路径。如果移动对象的运动复杂或随机性较强，卡尔曼滤波的效果就会大大降低。

- 粒子滤波：同样包含测量模型和动态模型，但不要求动态模型是线性的，可是对于噪声滤波的处理效率不够明显

#### b. 停留点检测
在某一时间区域或空间区域内产生某种行为的轨迹数据定义为停留点，一般将停留点分为**轨迹中停留点和环绕轨迹停留点**，停留点检测技术参考相关论文

### 2. 轨迹压缩
基于时间戳的轨迹记录可以采用秒级记录，但是存储空间和算力有限，通常需要对轨迹采取压缩的方法

- 垂直欧氏距离：利用每个点到首尾两端点连线的垂距进行筛选
- 时间同步欧氏距离：假设一个初始轨迹有n个采样点，视为有n-1个分段，A，B和C是三个连续的时空位置点，根据它们计算的时间同步欧式距离$$sed(A,B,C)=\sqrt{(x^{'}_B-x_B)^2+(y^{'}_B-y_B)^2}$$
  其中，$x_B^{'}=x_A+v_{AC}^x\times (t_B-t_A), y_B^{'}=y_A+v_{AC}^y\times (t_B-t_A)$，v是速度向量
  $$ v_{AC}^x=\frac{x_C-x_A}{t_C-t_A},v_{AC}^y=\frac{y_C-y_A}{t_C-t_A}$$
  

# 2. 基于分布式系统的高速数据流的动态查询

时空轨迹数据具有多维性、规模大的特点，同时因为数据分析的需求，对轨迹数据进行高速查询是必须面对的问题
近十年来，人们致力于将大规模的空间数据放置在商用集群机上进行管理与分布式计算。然而,传统的分布式数据处理系统（如Hadoop等）主要针对**离线批处理**设计，即需要先将数据全部收集和存储，然后再批量处理。这种模式不适合具有实时性要求的时空数据处理，因为它缺乏对时间属性的支持。

基于Hadoop的系统每次读写数据都要经过磁盘，存在大量的磁盘交互，不适合实时查询处理，而最近的一些系统，如Apache Flink和Spark Streaming，采用了基于内存的计算模式，将数据加载到内存中进行处理，大大减少了对硬盘的依赖。

最近提出的SPEAR架构中，就采取了Apache Flink作为流处理框架构建其基于内存的分布式引擎，在Flink提供的时间管理、事件管理等功能的基础上扩展增加了数据分区和索引、空间数据库等功能模块，在轨迹数据流的高速动态查询中展现出优秀的性能。

## SPEAR架构的细节
![[Pasted image 20241115083036.png]]
### 1. 流式时空数据类型
传统的空间维度通常是根据给定参考系的多个坐标轴来定义的，例如使用$(x,y)$表示点，两个端点表示一条直线
在空间维度基础上加入时间维度就变成了$ST(SpatialDim_{(x,y)},t)$，在SPEAR中，我们使用分布式流状上下文结构进行建模：
$$ StreamingST_{(n)}=ST^i(SpatialDim_{(x_i,y_i)},t_i)\quad  |\quad i \in [1,n]$$
也就是一个时间序列的形式，含有直到$t_n$的时空数据

### 2. 流式时空函数
传统的时空查询方法在查询对象发生变化或移动时，必须由用户或系统自动重新追踪。
而对于流式时空数据类型，一段连续的数据流就能说明查询对象处于移动状态。基于这一点，SPEAR提供了时空范围查询和最近邻查询，但与被分割至多台机器里并行工作的分布式环境中的数据流不同，SPEAR通过一系列连续且未被分割的数据流来定义移动的被查询对象，并且将其复制到所有分布式工作机器上，并行地

SPEAR时空查询算法大致可以归纳为：
设数据流为D，被查询对象为Q，时间窗口为T，时空查询的结果为r
- 将D分割至T个窗口中，即将数据流按照时间进行划分，每个窗口的数据为d
- 进行全局轻量级过滤：排除不可能符合查询条件的数据，是个快速的筛选操作
- 对于每个时间窗口T中的数据段d，进行GeoHash分区，将数据划分到不同网格中，便于进行后面的空间查询
- 对于分区后的每个格网中，并行地对于格网中的对象进行查询，若是查询对象满足查询要求，加入结果集r
- 遍历完毕后返回查询结果集r即可

### 3. 运行引擎
SPEAR被设计为在并行和分布式环境中运行，其引擎是基于Flink系统构建的。Flink提供了许多操作符来定义和处理连续数据流上的可以被用户自定义的函数，包括但不限于时间分配、窗口管理、状态处理等

## SPEAR时空分区的细节
现实世界分布式应用的主要瓶颈之一是数据分布不均匀或数据倾斜，即**数据没有均匀分布到各个节点上，导致某些节点的负载显著高于其他节点**
传统的数据倾斜缓解方法例如
- 键分区重新分配：将数据使用键值进行划分，将出现频率较高的那些键值代表的数据平均分配到不同的结点中，使负载均衡
- 随机分区：将数据随机划分到不同结点进行并行计算
- 哈希分区：对数据的键值进行哈希，使用哈希值决定应该分配到哪个结点中
- 分治处理：将大数据划分为小数据，在最后进行合并

很显然，由于时空数据的空间相关性和时间相关性，特定的空间网格或时间窗口内的数据会更加集中，并且数据的维度一般很高，还会动态变化，无法通过传统手段进行简单的静态分区
为此，SPEAR提出了一种**基于GeoHash的动态分区策略**，以适应分布式时空数据的负载

### 1. 动态分区
SPEAR将时空数据流以逻辑空间和时间单位的形式来划分、管理，这些就是被分配至不同分布式处理单元中进行并行处理的分区

解释一下SPEAR动态分区的理念：
对于输入的时空数据流，首先被按照逻辑时间窗口进行划分，给定时间内，系统中有且仅有一个时间窗口被处理，从而确保数据的处理仅在这一段时间内进行；每个逻辑时间窗口内的空间数据按照GeoHash进行分区，这些分区就是不同工作单元并行处理的对象
为了避免数据倾斜，SPEAR在每个时间窗口中维护一个partitionFactor，即每个工作单元中所处理的总的记录数量。
若这个值较低，说明少量工作单元获得了大量记录，出现数据倾斜，那么加大GeoHash的精度，产生更多分区用于给更多工作单元处理；同时还会检测每个工作单元所分配到的分区数量，在数量过大时降低GeoHash的精度

## SPEAR实验结果以及存在的问题
现有的除SPEAR以外的时空数据系统的比较：
![[Pasted image 20241114221437.png]]
拥有最佳性能的是同样基于Flink的MobyDick系统

对于静止对象的查询，SPEAR系统和MobyDick系统性能相当，这是因为它们都使用了java topology suite库进行空间计算
SPEAR框架的优越性在于对于移动对象的实时查询，支持动态调整以适应查询对象的移动，未来的工作将扩展现有的框架，以支持连续数据流上的时空连接等查询。

## Apache Flink
Flink框架从设计上就是为了优化处理实时流数据的，该框架提供了原生的流处理模型，其所有批处理操作也是基于流的方式来处理的，并且能够基于事件时间进行精确的时间窗口操作，拥有强大的时间处理能力

# 3. 面向大轨迹数据的分布式NoSQL存储引擎
轨迹数据的查询形式多样，例如：1)ID时态查询用于获取给定移动对象在指定时间范围内的轨迹信息；2)空间范围查询，查找经过给定空间范围的轨迹，可以发现可达区域；3)相似性查询，返回与给定轨迹相似的轨迹，可以帮助警察发现街道非法停车；4) k- nn（最近邻）查询，即找出与给定轨迹最相似的k条轨迹，可以研究驾驶习惯

为了支持不同形式的查询，需要一个可扩展性强的统一轨迹存储引擎。集中式的存储框架显然无法支持数量庞大的轨迹数据，但现有的分布式轨迹数据管理框架依然存在着一些性能问题：基于内存的分布式数据管理框架虽然将磁盘中的数据预取到内存中，避免频繁的I/O经过磁盘导致性能拖慢，然而这也要求数据被提前加载至内存，对于存储器的要求较高，成本昂贵，可扩展性不高。同时对于每个查询请求，需要发起一系列的总线事务，在存储器内进行索引与查找，性能有所限制

在传统的关系型数据库SQL之外，本文提出了使用索引关系更为多样的NoSQL式存储引擎来对轨迹数据进行存储。GeoMesa是一个开源的基于NoSQL的分布式存储大尺度时空数据的工具，它将多维的时空数据压缩至一维键值对数据进行索引，但是并不能直接应用于轨迹管理

因此，提出一种基于GeoMesa的轨迹数据存储引擎，TrajMesa：
![[Pasted image 20241115115358.png]]
其框架主要分为三个部分：
 - 预处理模块：包括噪声过滤、停留点检测以及轨迹分段操作
 - 索引与存储模块：利用GeoHash等方式，获得唯一键值，并且将数据存储在多个索引表当中。具体来说，生成了两个不同的键来索引一个轨迹的两个副本，在之后叙述
 - 查询模块：包括ID时间查询、空间范围查询、相似度查询和k近邻查询

## 3.1 背景知识

1. 轨迹：轨迹是一系列按照时间戳排列的GPS点，即：$$ \begin{align}
   tr &= \{p_1\rightarrow p_2 \rightarrow \cdots \rightarrow p_n\}\\
   p_i &= (lat_i, lng_i, t_i)
   \end {align}$$
2. ID时间查询：给定一个数据集$T$，一个移动的物体$oid$，以及一个时间范围$R=[t_s,t_e]$，ID时间查询返回$tr_i \in T$，其中$tr_i.oid = oid$，即给定时间范围R内物体的移动轨迹，包含多个GPS点

3. 空间范围查询：给定一个数据集$T$，一个空间范围$S=\{lat_{min},lng_{min},lat_{max},lng_{max}\}$，空间范围查询返回$tr_i\in T$，代表空间范围S内的一个或多个GPS点

4. 相似查询：给定数据集$T$，一个查询轨迹q，一个距离函数f，距离阈值为$\epsilon$，相似查询查找满足$tr_i$与$q$之间距离不大于$\epsilon$的$tr_i\in T$

5. k近邻查询：给定数据集$T$，查询q，正整数k，距离函数f，那么k近邻查询返回一系列轨迹$T^{'}\subseteq T$，其中$|T^{'}|=k$，并且对于$tr_i\in T^{'},tr_j\in T^{'}/T$，有$f(q,tr_i)<f(q,tr_j)$（即最近的k个邻居）
   若q是一个轨迹，那么f可以是$f_F$或$f_H$，称为k-NN轨迹最近邻；若q是一个点，f可以被定义为$f_{P}(q,tr)=\min\limits_{P_j\in tr}d(q,p_j)$，其中d代表两个点之间的欧几里得距离，这称为k-NN点查询

## 3.2 索引与存储
一种存储模式为**垂直存储模式**。一个将轨迹数据存储在一对键值对中的基本思想是：轨迹数据的每个GPS点被单独存储在一行中，将每个点视为一个独立的对象，存储其三维信息、属性等，使用关键字来索引
但是显然这样的方法并不适合轨迹数据，首先存储量占用大，并且数据关联性差，对于相似和近邻查询的效果不好，会触发更多的磁盘IO

本文提出一种新的**水平存储模式：将每条轨迹单独地存储在一行中**：
![[Pasted image 20241115193001.png]]
每个条目包含：
- 时空属性：轨迹的MBR（最小外接矩形），起止时间，起止位置
- GPS点列表：对轨迹中的GPS点先使用Kryo进行序列化，然后使用GZip进行压缩，降低存储成本以及磁盘I/O的次数，提高存储与查询效率
- 签名：大多数情况下，轨迹只占其MBR的很小一部分，最小外接矩形并不能准确表示其位置，为此，本文将轨迹的MBR进一步以$a\times a$的尺寸细分为子矩形，每个区域被编号，使用$a\times a$位的二进制序列进行标签：若一个或多个GPS点位于这个区域内，标为1，反之标为0。 a越大越精细，但是所需查询时间与存储空间也越多，这里取a=4
- 其他相关属性也一并存储在表项中

### 创新点：ID时间索引
为了支持高效的ID时间索引，该系统使用