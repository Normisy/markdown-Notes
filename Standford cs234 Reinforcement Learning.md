# Lecture 0.  绪论
## RL介绍与对比
强化学习指的是通过经验/数据学习，在不确定的环境下做出良好决策。对于强化学习来说，它的核心要素可以被归纳为：
1. 优化：找到最优的决策方式，需要产生最佳或者至少很好的结果，具有明确的决策效用概念
2. 延迟后果：当前所做的决策可能会在很久之后产生效用，因此这样的要素使得决策需要考虑不仅仅是即时收益,还要考虑**长期影响**；并且对于**时间信用**的分配很困难(是什么导致了后来的高奖励或低奖励?)
3. 探索：智能体通过感知-决策-行动来了解世界，反复行动进行学习
4. 泛化generalization：策略是由过去经验到动作的映射，而在强化学习中，我们希望做出决策的状态空间非常庞大，我们不可能单纯使用规则来将它们全部描述出来。

强化学习和其他的机器学习方法的差别如下表所示，叉号表示“具有对应的功能“
![[Pasted image 20251102144242.png]]
实际上，很多不同的机器学习方法可以互相转换，也就是"General泛化、简化“功能。例如模仿学习IL能够通过收集良好的演示案例，将强化学习转换为监督学习，从而继承其他领域中已有的方法论
（通常来说，即使模仿学习的数据非常优秀，但强化学习获得的性能往往会大于等于模仿学习）

什么情况下强化学习会比其他机器学习方法更优秀？其中一种情况就是你没有满意的案例，我们希望模型能够超越人类的表现，而非仅仅学习人类是怎么做的。这也就是“optimization”，强化学习能够解决具有延迟后果的巨大搜索或优化问题，例如DeepMind的AlphaTensor就利用智能体强化学习发明了比已知更快的矩阵乘法和排序算法，这本质上是一个Planning规划问题被简化成了强化学习问题
## 问题建模
在正式进入理论学习之前，我们考虑一个问题

> [!QUESTION] 如何建模一个决策过程为强化学习问题（马尔可夫决策过程）？
> 假设我们有一个AI家教，它负责提供加法或减法练习题以教会学生加减法。每次学生答对智能体出的题目，奖励+1；反之奖励-1。在这样的条件下：
> - 状态空间是什么？
> - 动作空间是什么？
> - 奖励模型是什么？
> - 动态模型是什么？
> - 为了优化（最大化）预期折现总奖励，策略会在该场景下做些什么？

这是一个开放性的问题，因此答案不唯一，但课程给出了一些解释。
- 状态空间：
	- 不妨使用一个向量对$[num_1, num_2]$来表示学生在加法和减法上的掌握程度，数值越低表示对应知识的掌握程度越低，该方法很自然，尤其是对于熟悉隐马尔可夫模型的人来说
	- 另一种好方法是直接将之前所有的“题目+学生回答+奖励”历史记录序列作为状态，在一定的表述方式下，它和之前的掌握程度向量对是可以同构的，但是存在无限增长的问题
- 动作空间：很简单，就是提供加法或减法练习题
- 奖励模型：学生答对获得奖励+1，答错则奖励-1
- 动态模型：动态模型描述的是学生在被给出算术题目之后，其内部状态（知识掌握程度或历史知识）的变化情况。从一个状态开始，执行一个动作，过渡到另一个新的状态。在这个题目中，我们很自然地可以认为学生回答了一道加法或减法题，那么他对加法或减法的掌握程度就会对应提高
- Reward Hacking：如果一个策略追求最大化奖励，也就是尽量保证学生正确回答，那么智能体可能只会不断地给出非常简单的题目以确保它能不断获得奖励，但学生无法学习到更难的知识，与系统的设计初衷相悖

## 序贯决策与马尔可夫假设
不确定性下的序贯决策：智能体作出一系列决策（行动），与现实世界交互，获得一个观察和奖励信号，其目标是最大化预期的未来总奖励。
例如，对于视频广告播放机制，其目的是最大化用户的观看时长或广告收入；对于机器人学，则可能是机器人的行动程度，但是奖励劫持可能会导致机器人作出不符合我们预期的行为，因此奖励的设计至关重要
时间步长：一般我们假设决策是在**有限的离散时间步长序列**${t_1, t_2, \dots}$中进行的，每隔一定步长t就会执行一次决策$a_t$（本课程**不考虑连续时间情况**），然后世界根据该决策进行一次更新，发出观察结果$o_t$和奖励$r_t$，智能体执行下一个决策
历史记录相应地表示为$h_t=(a_1,o_1,r_1,\dots, a_t, o_t,r_t)$，记录了**迄今为止的所有信息**，但我们也会希望存在其他信息量更大的表示，以**简化表示冗长的历史信息**，使得问题更容易处理

因此，在序贯决策中，我们会期望一个状态含有足够多的历史信息，通常会作出**马尔可夫假设**：当前状态$S_t$能够充分地总结所有历史信息，使得未来只依赖于当前状态，而无需考虑更早以前的历史状态（马尔可夫假设在给定当前状态情况下进入下一个状态的概率等于在给定所有历史记录情况下进入下一个状态的概率。$p(s_{t+1}|s_t, a_t) = p(s_{t+1}|h_t,a_t)$）“给定现在，未来独立于过去”
这样一来，只要对当下的情况足够了解，就能不必考虑过去的历史；同时，我们也会考虑尽量缩小状态空间，例如健康检测不需要考虑这个人从出生开始的所有健康数据，只需要考虑一天以内的就能够做出下一个决策了

然而，尽管马尔可夫决策非常简单而且经常有效，能够降低计算和存储的需求，但是过于简化的状态可能无法完全捕获世界的复杂性，限制最终的性能，这是一个速度-性能的权衡问题
因此，在我们建模一个决策过程时，首先需要问的问题是“状态是马尔可夫式的吗？" "世界是否是部分可观测的：存在智能体无法直接观察到的真实潜在状态，例如学生的内在知识（本课程不会深入讨论这样的问题）"  “动态过程是确定性的还是存在随机性？”  “行为只影响即时奖励，还是会影响下一阶段的状态？“

## 马尔可夫决策过程（MDP）
动态模型告诉我们当我们做出决策时，状态是如何改变的。我们并不一定每次都能看到它，但是它确实存在，而且我们允许随机性的存在。
因此，我们使用概率分布来描述我们进行一个行动后，下一个到达状态的概率是多少：
$$ P(s_{t+1}=s^{'} | s_t=s, a_t=a)$$
代表给定当前行动$a_t=a$和状态$s_t=s$的情况下，下一刻的状态为$s^{'}$的条件概率
该情况下，即时的预期奖励模型被表示为：
$$ r(s_t=s, a_t=a)=\mathbb{E}[r_t|s_t=s, a_t=a]
$$
这就是基于模型的马尔可夫决策过程，其中的概率分布、奖励模型等都由模型给出，模型代表了智能体看待世界的方式，但是模型可能是错误的，从而影响智能体的性能
在这种情况下，我们使用$\pi$来表示策略：智能体选择动作的方式
决策策略本质上就是从状态到动作的映射，即
$$ \pi(s)=a$$
这是确定性的策略。
也存在随机策略：
$$ \pi(a|s)=Pr(a_t=a|s_t=s)$$
策略并不是一成不变的，一般会从一个起始状态开始持续演变，从而找到一个最佳的策略

### 马尔可夫奖励过程（MRP)
实际上就是马尔可夫链+奖励，包括：
- S代表一个确定的状态集合$s\in S$
- P是一个动态/转移模型，确定$P(s_{t+1}=s^{'}|s_t=s)$
- R是一个奖励函数$R(s_t=s)=\mathbb{E}[r_s|s_t=s]$，用于描述每种状态的好坏程度，若状态是有限的，R可以被写成一个向量，每个位置上代表每个状态对应的奖励值
- 折扣因子$\gamma\in [0,1]$，之后会提到
在这些条件下，我们还需要思考一个特定的转换轨迹的好坏程度，以帮助我们做出优秀的决策。
- H代表视界，即每个决策事件所具有的时间步总数，可以是有限的也可以是无限的，它相当于告诉我们有多少时间步长用于决策
- $G_t$代表回报，即从当前时间步到视界结束的奖励的折扣总和
$$ G_t = r_t+\gamma r_{t+1}+\gamma ^ {2}r_{t+2} + \cdots + \gamma ^{H-1}r_{r+H-1}$$
- 一个马尔可夫奖励过程的价值函数V(s)就是预期的回报：
$$ V(s)=\mathbb{E}[G_t|s_t=s]=\mathbb{E}[r_t+\gamma r_{t+1}+\gamma ^ {2}r_{t+2} + \cdots + \gamma ^{H-1}r_{r+H-1} | s_t=s]$$
在随机过程中，它只是一种期望，和实际回报不一定相同；它代表了你从当前状态开始一直按照某个决策进行下去，最后能够预期得到多少回报

我们可以注意到这里出现了折扣因子$\gamma\in [0,1]$，公式中将它随着时间步退后而增加幂次，代表了我们更看重早期的回报，更不看重晚期的回报，是一种权重。它的作用一方面是在数学上避免求和不收敛（当然，在有限时间步长的情况下可以固定$\gamma$，无需考虑收敛）；另一方面是注重当下收益符合人类的思考方式，在无穷马尔可夫奖励过程中更是如此。
同时可以注意到，当$\gamma$从0到1递增时，它代表我们逐渐重视长期回报（即时间步靠后的那些回报），最大值是1，代表我们平等地重视任何时间步中的回报。

# Leture 1. Tabular MDP Planning
## 1. MRP中的Bellman更新算法（Bellman Backup）
如前所述，当我们谈到“世界模型”的时候，本质上指的是一个指示当我们做出决策后状态如何改变的动态模型，以及一个奖励模型。我们期望找到最优的决策，这意味着模型需要能够评判一个决策的好坏，并且理解什么是最优的，这就是价值函数的由来。

在前文所叙述的无限视界马尔可夫奖励过程的情况下，我们利用马尔可夫过程的性质（未来的状态只受当前状态控制，而不考虑过去的状态），可以如下计算其价值函数V(s)，此处固定了策略：
$$ V(s)=\underbrace{R(s)}_{即时奖励}+\underbrace{\gamma\sum_{s^{'}\in S} P(s^{'}|s)V(s^{'})}_{具有折扣因子的未来奖励总和}$$
这里R(s)是行动过后立刻得到的即时奖励，第二项则是接下来可能达到的所有不同的未来状态$s^{'}\in S$的价值（未来奖励的期望值，使用条件概率加权），该公式是之后贝尔曼方程的基础

对于有限状态的MRP，我们可以使用矩阵形式来重写其价值函数：
$$ \begin{bmatrix}
V(s_1)\\
\vdots\\
V(s_N)
\end{bmatrix} = 
\begin{bmatrix}
R(s_1)\\
\vdots\\
R(s_N)
\end{bmatrix} +
\gamma
\begin{bmatrix}
P(s_1|s_1) & \cdots & P(s_N|s_1)\\
P(s_1|s_2) & \cdots & P(s_N|s_2)\\
\vdots\\
P(s_1|s_N) & \cdots & P(s_N|s_N)\\
\end{bmatrix}

\begin{bmatrix}
V(s_1)\\
\vdots\\
V(s_N)
\end{bmatrix}
$$
即为每个状态单独维护一个标量值，$V=R+\gamma PV$，P就是转移矩阵（有时也使用T指代）
进一步可以推导得到$V=(I-\gamma P)^{-1}R$，这里$I$是单位矩阵，可见如果世界模型有限且足够小，是可以直接计算得到每个状态的价值函数的（解析解），但是因为需要矩阵求逆，其计算成本会很大（且要求转移矩阵可逆）

因此，大多数情况下都不能直接求出价值函数的解析解，于是我们不直接进行分析，考虑使用动态规划，设计一个迭代算法来求取数值解
我们的想法是尽量避免矩阵求逆，因为其成本和要求都很高。既然无法一步到位，我们不妨“**先猜测、然后一步步进行修正**，先看算法：
1. 初始化每个状态$s$的价值函数猜测值$V_0(s)=0$
2. 将循环因子k从1开始迭代（直到价值函数猜测值不变或到达指定精度）：
	- 对于所有$s\in S$，修正猜测值：
		$V_k(s)=R(s)+\gamma \sum_{s^{'}\in S}P(s^{'}|s)V_{k-1}(s^{'})$
3. 计算到最后，当价值函数猜测值$V_k(s)$保持不变，就得到了所有状态的价值函数
整个算法的每次迭代（猜测）的时间复杂度为$O(N^2)$，N是状态的总数，迭代次数取决于后面会提到的收敛速度，我们先看算法在做些什么。

该算法首先
###  数学证明：收敛性（不动点）
#### 证明1：压缩映射
定义**Bellman算子** T： 
$$[TV](s) = R(s) + \gamma \sum_{s'} P(s'|s) V(s')$$ 我们的迭代就是：$V_{k+1} = TV_k$ 
**关键定理（压缩映射定理）**： 
**引理**：T是一个γ-压缩映射，即： $$\|TV_1 - TV_2\|_{\infty} \leq \gamma \|V_1 - V_2\|_{\infty}$$ **证明**： $$|[TV_1](s) - [TV_2](s)| = \gamma \left|\sum_{s'} P(s'|s)[V_1(s') - V_2(s')]\right|$$ $$\leq \gamma \sum_{s'} P(s'|s)|V_1(s') - V_2(s')|$$ $$\leq \gamma \sum_{s'} P(s'|s) \|V_1 - V_2\|_{\infty}$$ $$= \gamma \|V_1 - V_2\|_{\infty}$$ 由于 $\gamma < 1$，T是压缩的！ **Banach不动点定理**保证： 
1.  存在**唯一不动点** $V^*$，使得 $TV^* = V^*$ 
2.  从任意 $V_0$ 开始迭代，序列 $V_k = T^k V_0$ **必然收敛**到 $V^*$ 
3. 收敛速度：$\|V_k - V^*\|_{\infty} \leq \gamma^k \|V_0 - V^*\|_{\infty}$（指数衰减） 
#### 证明2：误差单调递减
设真实价值为 $V^*$，第k步误差为 $\epsilon_k = V_k - V^*$ $$V_{k+1} - V^* = TV_k - TV^* = T(V_k - V^*) + TV^* - V^*$$ 由于 $V^*$ 是不动点，$TV^* = V^*$，所以： $$\epsilon_{k+1} = T\epsilon_k$$ 应用压缩性质： $$\|\epsilon_{k+1}\|_{\infty} \leq \gamma \|\epsilon_k\|_{\infty} \leq \gamma^{k+1} \|\epsilon_0\|_{\infty}$$ **结论**：误差以 $\gamma^k$ 的速度指数衰减到0，即原式收敛到真实值的速度

## 2. MDP马尔可夫决策过程
在之前的基础上，我们加入行动要素，此时需要真正地进行决策，它其中含有：
- S是一个（有限的）马尔可夫状态集合，$s\in S$
- A是一个（有限的）行动集合，$a\in A$
- P是对于每个行动的**动态/转换模型**，即$P(s_{t+1}=s^{'}| s_t=s, a_t=a)$，规定了**给定初始状态和行动的情况下，下一步状态为$s^{'}$的条件概率**。在P的表示中，**有限情况下**，往往固定$a_t=a$，然后使用一个矩阵来描述，例如假设一个系统具有两种状态，两种行动，那么$P(s^{'}|s, a_1)=\begin{bmatrix} / & {a_1} & {a_2} \\ a_1 & 0.3 & 0.7 \\ a_2 & 0.4 & 0.6 \end{bmatrix}$代表执行行动1后，从每个状态转移到其他状态的概率矩阵，因此该矩阵一定是一个$N\times N$矩阵（N是状态总数），并且每一行的元素相加值一定为1（概率和为1）
- R是一个奖励函数，$R(s_t=s, a_t=a) = \mathbb{E}[r_t|s_t=s, a_t=a]$
- 折扣因子$\gamma \in [0,1]$
因此可以直接使用元组$(S, A, P, R, \gamma)$来定义一个马尔可夫决策过程

## 3. MDP+策略=MRP
### 推广奖励和动态模型至策略情况
现在我们继续将策略$\pi$加入考虑，这是整个课程中都会讨论的重点，即如何根据智能体所处于的状态做出策略
策略具体说明将要采取的行动，可以是随机性的或是确定性的。我们通常将其视为随机的，即$\pi (a|s)$代表**在状态s下采取行动a的条件概率**
一个马尔可夫奖励过程也可以视为是一个马尔可夫决策过程加上策略之后的结果。这是因为当我们确定需要如何行动之后，为了量化这项策略的好坏，我们需要**将奖励推广到期望情况**，即在具有策略的情况下，固定状态s后奖励等于该状态下每种行动的奖励乘上决策该状态的条件概率$\pi (a|s)$后的加权和
$$ R^{\pi}(s)=\sum\limits_{a\in A}\pi(a|s)R(s,a)$$
$a\in A$代表了所有可能做出的行动集合，集合内每个元素对应的被选中的概率就是$\pi(a|s)$

该情况下，动态模型稍微复杂一些：
$$ P^{\pi}(s^{'}|s)=\sum\limits_{a\in A}\pi(a|s)P(s^{'}|s,a)$$
因为动态模型是给定初始状态下，转移到其他状态的概率模型，对它的推广就是将给定初状态和每种动作的动态模型加权$\pi(a|s)$的加权和，这里$s^{'}$是自变量，代表转换的对应目的状态
显然，该写法也可以写成矩阵形式，将$P^\pi$和P都表示为$N\times N$的转换概率矩阵也可以（有限情况下）
这样，一个MRP就可以使用四元组$(S, R^\pi, P^\pi, \gamma)$来定义，推广以后我们就能够使用相关的方法评估一个马尔可夫决策过程中的策略好坏了，也就是**价值函数**

### 推广价值函数更新算法（特定策略的贝尔曼backup算法）
将之前的bellman backup算法推广到具有策略的情况：
1. 初始化所有状态s的第一次猜测值$V_0(s)=0$
2. 将循环变量k从1开始往后遍历
	1. 对于所有$s\in S$，修正其猜测值为：
		$V_k^{\pi}(s)=\sum\limits_{a}\pi(a|s)[R(s,a) + \gamma \sum\limits_{s^{'}\in S}P(s^{'}|s,a)V_{k-1}^\pi(s^{'})]$
3. 当猜测值$V_k^\pi$保持不变或者到达需要的精度时，最后的V就是价值函数数值解

注意到当策略是确定性（情况s下有且只有一种动作，直接映射）的时，$\pi(a|s)=1,a=\pi(s)$，更新公式可以写成：$V_k^{\pi}(s)=R(s,\pi(s)) + \gamma \sum\limits_{s^{'}\in S}P(s^{'}|s,\pi(s))V_{k-1}^\pi(s^{'})$
## 寻找最优策略
显然，一个强化学习问题中的状态总数为$|A|^{|S|}$，A和S分别是行动集和状态集。如果多个行动具有相同的值，那么最佳策略可能不唯一。
在MDP控制中，我们所计算的优化策略即为
$$ \pi^{*}(s)=\arg\max_{\pi}V^{\pi}(s)$$
即在所有的策略$\pi$中，寻找一个映射$\pi:A \rightarrow S$，使得动作s能够最大化价值函数。在正式学习如何计算它的时候，作为一个结论，需要知道：
1. 一个MDP过程中，存在唯一的最优价值函数，即无论从哪种策略开始优化，最终收敛到的价值函数都是同一个（不动点）
2. 对于无限时间范围的MDP问题，最优策略是确定性的，而非一个动作的概率分布
在这两个条件下，我们