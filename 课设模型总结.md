## 一、数据预处理 (`preprocessing.py`)

### 1.1 地理空间过滤

**目的**：将Yelp原始数据转换为可用于分析的高质量数据集，确保后续分析的准确性和可靠性。通过多层次过滤和清洗，从原始数据集中提取目标城市的餐厅数据样本。

**方法**：首先通过**城市名称和州代码**索引城市，避免同名城市混淆；同时利用**关键词筛选**方法，在海量商户中准确识别餐饮业，排除酒店、商场等非餐饮业态

**基于行政区划和业态关键词的双重过滤**，首先固定行政区划：
```python
CITY_NAME = "Philadelphia"  
CITY_STATE = "Pennsylvania"  
CITY_COUNTRY = "USA"
```

然后关键词匹配筛选：
```python
# preprocessing.py
restaurant_keywords = [
    'restaurant', 'food', 'cafe', 'bar', 'pizza', 'chinese',
    'mexican', 'italian', 'sushi', 'burger', 'bakery', 'diner'
]

def is_restaurant(categories):
    if pd.isna(categories):
        return False
    categories_lower = str(categories).lower()
    return any(keyword in categories_lower for keyword in restaurant_keywords)
```

**作用**：确保只分析餐饮行业商户，排除其他类型商业实体

### 1.2 坐标质量控制

**目的**：保证餐厅地理坐标的准确性和有效性，这是后续空间分析的基础。错误或缺失的坐标会导致网络匹配失败、可达性计算错误、聚类结果失真。

**方法**：基于城市边界的几何验证

```python
# preprocessing.py
valid_coords_mask = (
    restaurants_df['latitude'].notna() &
    restaurants_df['longitude'].notna() &
    (restaurants_df['latitude'] != 0) &
    (restaurants_df['longitude'] != 0) &
    (restaurants_df['latitude'] >= LAT_MIN) &
    (restaurants_df['latitude'] <= LAT_MAX) &
    (restaurants_df['longitude'] >= LON_MIN) &
    (restaurants_df['longitude'] <= LON_MAX)
)
```

**作用**：移除"零岛"坐标、缺失值和超出城市边界的异常点，确保后续空间分析的准确性

### 1.3 文本标准化

**目的**：统一评论文本格式，便于主题建模

**方法**：停用词过滤 + 小写转换 + 标点符号移除

**停用词过滤**：移除高频但无实际意义的功能词，保留有区分度的内容词
**停用词集合** $S$：包含常见无意义词汇（约50个）
```python
S = {'i', 'me', 'my', 'we', 'the', 'a', 'an', 'and', 'but', 'or', 
     'is', 'was', 'are', 'were', 'to', 'from', 'in', 'on', ...}
```

统一转化为小写，并移除标点符号：
```python
# preprocessing.py
def clean_text(text):
    text = str(text).lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    words = text.split()
    words = [word for word in words if word not in STOP_WORDS and len(word) > 1]
    return ' '.join(words)
```

**作用**：降低文本噪声，提高LDA主题模型的质量

---

## 二、主题建模 (`LDA.py`)

### 2.1 LDA (Latent Dirichlet Allocation)

**目的**：发现评论文本的潜在主题结构，假设每条评论是多个主题的混合，每个主题由一组相关词汇表征。
从用户评论中提取潜在主题，构建餐厅语义画像

**算法**：Gensim库实现的LDA概率主题模型

**核心参数**：
固定LDA模型的主题数量为6个：食物质量、服务、环境、性价比、位置、特色体验


```python
# LDA.py
lda_model = LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=6,           # 主题数量
    random_state=42,
    passes=10,              # 训练轮数
    iterations=50,          # 每轮迭代次数
    alpha='auto',           # 文档-主题分布先验（自动优化）
    eta='auto',             # 主题-词分布先验（自动优化）
    per_word_topics=True
)
```

**作用**：将每家餐厅映射到6个主题维度（食物质量、服务、环境、性价比、位置、特色体验）

#### 2.1.1 数学模型（以防万一）

**生成过程假设**：

对于文档（评论）$d$：

**步骤1**：从 Dirichlet 分布中采样文档的主题分布 $$\theta_d \sim \text{Dir}(\alpha)$$

**物理意义**：决定这条评论会有多少比例谈食物、多少比例谈服务

**步骤2**：对于评论中的每个词 $w_{d,n}$：

- **2a**. 从主题分布中采样该词的主题： $$z_{d,n} \sim \text{Multinomial}(\theta_d)$$
    
    **物理意义**：这个词是在讨论哪个主题？
    
- **2b**. 从该主题的词分布中采样具体的词： $$w_{d,n} \sim \text{Multinomial}(\phi_{z_{d,n}})$$
    
    **物理意义**：在"食物"这个主题下，生成"delicious"这个词
    

**参数说明**：

|参数|含义|本项目设置|作用|
|---|---|---|---|
|$K$|主题数量|6|决定主题的粒度|
|$\alpha$|文档-主题先验|auto（自动优化）|控制主题分布的稀疏性|
|$\eta$ (beta)|主题-词先验|auto（自动优化）|控制词分布的稀疏性|
|$\theta_d$|文档 $d$ 的主题分布|维度为 $K$|每条评论的主题配方|
|$\phi_k$|主题 $k$ 的词分布|维度为 $V$|每个主题的词汇特征|

**为什么$\alpha$和$\eta$选择auto**：

- 不同数据集的最优参数差异大
- 自动优化能根据数据特征调整
- 避免人工调参的主观性

#####  联合概率分布

完整的数据生成概率：

$$ P(W, Z, \theta, \phi \mid \alpha, \eta) = \prod_{k=1}^{K} P(\phi_k \mid \eta) \prod_{d=1}^{D} P(\theta_d \mid \alpha) \prod_{n=1}^{N_d} P(z_{d,n} \mid \theta_d) P(w_{d,n} \mid \phi_{z_{d,n}}) $$

**公式解读**：

- $\prod_{k=1}^{K} P(\phi_k \mid \eta)$：所有主题的词分布概率
- $\prod_{d=1}^{D} P(\theta_d \mid \alpha)$：所有文档的主题分布概率
- $\prod_{n=1}^{N_d} P(z_{d,n} \mid \theta_d)$：词的主题分配概率
- $P(w_{d,n} \mid \phi_{z_{d,n}})$：给定主题下生成词的概率

##### 参数推断

**目标**：从观测到的词汇 $W$ 推断隐变量 $Z, \theta, \phi$

**方法**：变分贝叶斯推断（Variational Bayes）

**核心思想**： 直接计算后验分布 $P(\theta, \phi, Z \mid W)$ 不可行（积分无解析解），用一个简单分布 $q(\theta, \phi, Z)$ 来近似。

**优化目标**：最大化证据下界 (ELBO) $$ \mathcal{L}(W \mid \alpha, \eta) = \mathbb{E}_q[\log P(W, Z, \theta, \phi \mid \alpha, \eta)] - \mathbb{E}_q[\log q(Z, \theta, \phi)] $$

**物理意义**：

- 第一项：数据的似然（模型拟合度）
- 第二项：近似分布的熵（模型复杂度惩罚）
- ELBO越大，模型越好

**训练参数**：

|参数|设置|作用|为什么这样设置|
|---|---|---|---|
|passes|10|完整遍历语料库10次|确保收敛，实验表明10轮充足|
|iterations|50|每轮内部迭代50次|平衡收敛速度和质量|
|random_state|42|随机种子|确保结果可重复|


### 2.2 语料库构建

**目的**：LDA模型需要事先将原始文本转换为LDA可处理的词袋模型，从原始文本中构建一个精简、高质量的词汇表，作为LDA模型的"词汇空间"。词典质量直接决定主题质量。

**方法**：词典过滤 + 文档向量化
1. 收集所有出现过的唯一词汇，建立 词 ↔ ID 的映射
2. 移除统计极端的词汇，保留信息量最大的核心词汇
3. 获得压缩之后的最合适的词汇表

```python
# LDA.py
# 创建词典并过滤极端词汇
dictionary = corpora.Dictionary(texts)
dictionary.filter_extremes(
    no_below=5,      # 至少出现在5个文档中
    no_above=0.5,    # 不超过50%的文档
    keep_n=5000      # 保留最常见的5000个词
)

# 创建语料库（词袋表示）
corpus = [dictionary.doc2bow(text) for text in texts]
```

**作用**：平衡词汇覆盖率和计算效率，移除过于罕见或过于常见的词

处理完成后，使用Bag-Of-Word（词袋模型）表示文本并作为模型的输入：将文本从词序列转换为词频向量，这是LDA模型的输入格式。BoW假设词的顺序不重要，只关注词的出现及频次。


### 2.3 主题聚合

**目的**：从评论级主题分布聚合到餐厅级主题画像

**方法**：组内均值聚合，整合每个餐厅的所有评论，将它们通过均值聚合聚合为一个唯一的主题向量，得到这家餐厅的画像

```python
# LDA.py
# 按餐厅分组,计算主题平均值
restaurant_topics = reviews_with_topics.groupby('business_id')[topic_columns].mean()

# 计算主导主题
dominant_topic_idx = topic_values.argmax(axis=1)
dominant_topic_prob = topic_values.max(axis=1)
```

**作用**：每家餐厅获得一个6维主题向量和一个主导主题标签
主导主题：按照主题向量每个维度的概率值划分
- 概率>0.5：主题非常突出，餐厅有明确定位
- 概率0.3-0.5：主题较突出，但也涉及其他方面
- 概率<0.3：无明显主导主题，餐厅较为均衡


---

## 三、网络可达性分析 (`network.py`)

### 3.1 街道网络下载

**目的**：获取城市交通网络拓扑结构

**工具**：OSMnx - OpenStreetMap数据处理库，下载所有道路类型进行处理

```python
# network.py
G = ox.graph_from_place(
    f"{city}, {state}, {country}",
    network_type='all',    # 包含所有道路类型
    simplify=True          # 简化网络拓扑
)
```

**作用**：构建后续可达性计算的基础网络

### 3.2 坐标系投影（关键修复）

**目的**：将WGS84经纬度转换为UTM投影坐标，确保距离计算准确

**方法**：Pyproj坐标转换器

```python
# network.py
# 创建坐标转换器：从WGS84到目标投影坐标系
transformer = Transformer.from_crs("EPSG:4326", target_crs, always_xy=True)

# 批量转换餐厅坐标到UTM
for lon, lat in zip(restaurants_df['longitude'], restaurants_df['latitude']):
    x, y = transformer.transform(lon, lat)
    projected_coords.append((x, y))
```

**作用**：避免直接用经纬度匹配投影网络节点导致的距离计算错误，确保节点匹配的准确性

### 3.3 步行可达性

**目的**：量化餐厅的步行友好程度，为网络的每条边（街道段）计算步行和驾车的行程时间，作为等时圈分析的基础。

**算法**：基于时间约束的网络子图提取 (Ego Graph)
使用`walk_time`衡量

```python
# network.py
# 步行时间（秒）= 边长度（米）/ 步行速度（米/秒）
data['walk_time'] = length / walk_speed_ms

# 提取10分钟步行可达的网络子图
subgraph = nx.ego_graph(
    G_proj,
    center_node,
    radius=600,              # 10分钟 × 60秒
    distance='walk_time'     # 使用步行时间作为距离度量
)

# 可达性指标
walk_accessibility_score = node_count + total_length / 1000
```

**作用**：评估餐厅周边人流可达性，分数越高表示步行可达范围越大

### 3.4 驾车可达性

**目的**：评估餐厅对驾车客流的吸引范围

**方法**：凸包面积估算



```python
# network.py
# 15分钟驾车等时圈
subgraph = nx.ego_graph(G_proj, center_node, radius=900, distance='drive_time')

# 用凸包估算可达区域面积
from scipy.spatial import ConvexHull
hull = ConvexHull(node_coords)
area_km2 = hull.volume / 1_000_000  # 转换为平方公里
```

**作用**：量化餐厅对汽车交通的依赖程度和服务范围

### 3.5 介数中心性

**目的**：识别位于交通要道上的战略位置

**算法**：网络介数中心性 (Betweenness Centrality)

python

```python
# network.py
betweenness = nx.betweenness_centrality(
    G_proj,
    k=1000,              # 采样1000个节点对
    weight='length',     # 使用物理距离作为权重
    normalized=True      # 归一化到[0,1]
)
```

**作用**：高介数中心性表示该位置是多条最短路径的必经之地，具有更高的曝光度

---

## 四、商业属性提取 (`feature.py`)

### 4.1 价格等级提取

**目的**：获取餐厅定价水平

**方法**：从商户attributes字段解析

python

```python
# feature.py
# 可能的字段名：RestaurantsPriceRange2, PriceRange
price = attributes.get('RestaurantsPriceRange2')
if price is None:
    price = attributes.get('PriceRange')
```

**作用**：价格是消费者决策的关键因素，用于后续竞争分析

### 4.2 菜系单热编码

**目的**：将分类变量转换为模型可用的数值特征

**方法**：One-Hot Encoding

python

```python
# feature.py
# 为每个商户创建单热编码
for category in top_categories.index:
    col_name = f"cat_{category.replace(' ', '_').lower()}"
    has_category = []
    for idx, row in business_df.iterrows():
        categories = row.get('categories')
        has_cat = category in str(categories)
        has_category.append(1 if has_cat else 0)
    
    category_columns[col_name] = has_category
```

**作用**：30个主要菜系类别转换为30维二值向量，保留菜系信息用于建模

### 4.3 缺失值处理

**目的**：保证特征完整性

**策略**：缺失指示器 + 中位数填充

python

```python
# feature.py
# 添加缺失指示器
data_clean['price_level_missing'] = data_clean['price_level'].isna().astype(int)

# 用中位数填充
data_clean['price_level'] = data_clean['price_level'].fillna(
    data_clean['price_level'].median()
)
```

**作用**：既保留了缺失信息（可能本身有意义），又不影响模型训练

---

## 五、空间竞争特征 (`space_features.py`)

### 5.1 情感极性聚类

**目的**：识别高评分和低评分的地理集聚区域

**算法**：DBSCAN（Density-Based Spatial Clustering）

python

```python
# space_features.py
# 组合特征：空间坐标（公里） + 情感极性
spatial_weight = 2.0
features = np.hstack([coords_km * spatial_weight, sentiment_scaled])

# DBSCAN聚类
clustering = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')
labels = clustering.fit_predict(features)
```

**作用**：发现"美食热点"和"冷门区域"，为选址提供参考

### 5.2 邻域竞争强度

**目的**：量化餐厅所处的竞争环境

**方法**：半径搜索 + 距离矩阵

python

```python
# space_features.py
# 计算1公里内所有餐厅的距离
dist_matrix = cdist(coords_km, coords_km, metric='euclidean')

# 找到邻域内的餐厅
neighbors_mask = (dist_matrix[i] < radius_km) & (dist_matrix[i] > 0)

# 竞争强度 = 邻居数量 × 平均评分
competition_intensity = n_neighbors * avg_stars
```

**作用**：识别"红海"（高竞争）和"蓝海"（低竞争）区域

### 5.3 主题差异化

**目的**：衡量餐厅主题的独特性

**算法**：余弦距离 (Cosine Distance)

python

```python
# space_features.py
# 计算主题向量之间的余弦距离
cosine_dist_matrix = cosine_distances(topic_vectors)

# 与邻居的平均余弦距离
topic_uniqueness = cosine_dist_matrix[i, neighbor_indices].mean()
```

**作用**：高独特性表示餐厅提供了差异化的体验，可能具有竞争优势

### 5.4 战略区域分类

**目的**：综合评估每个位置的商业价值

**方法**：多维度规则分类

python

```python
# space_features.py
def classify_zone(row):
    competition = row.get('competition_intensity', 0)
    avg_stars = row.get('neighbor_avg_stars', 3)
    uniqueness = row.get('topic_uniqueness', 0.5)
    
    if competition > quantile_75 and avg_stars >= 4.0:
        return 'Premium District'      # 高端区域
    elif competition > quantile_75 and avg_stars < 3.5:
        return 'Saturated Market'      # 饱和市场
    elif competition < quantile_25 and uniqueness > 0.6:
        return 'Niche Opportunity'     # 利基机会
    # ...
```

**作用**：将城市划分为5类战略区域，指导选址决策

---

## 六、两阶段预测模型 (`modeling.py`)

### 6.1 基线模型：线性回归

**目的**：使用基础商业属性建立预测基准

**特征**：价格、位置、菜系（约30-40个特征）

python

```python
# modeling.py
# 标准化特征
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# 训练线性回归
baseline_model = LinearRegression()
baseline_model.fit(X_train_scaled, y_train)
```

**作用**：提供可解释的基准预测，系数直接反映特征重要性

### 6.2 残差学习：LightGBM

**目的**：用高级特征（网络、主题、竞争）捕捉非线性关系

**算法**：梯度提升决策树 + 分位数回归

python

```python
# modeling.py
# 计算基线残差
residual_train = y_train - y_train_baseline

# 训练三个分位数模型（P10, P50, P90）
for quantile in [0.1, 0.5, 0.9]:
    params = {
        'objective': 'quantile',
        'alpha': quantile,
        'num_leaves': 31,
        'learning_rate': 0.05
    }
    
    model = lgb.train(params, train_data, num_boost_round=500)
```

**作用**：

- **P50模型**：提供中位数预测（主要预测）
- **P10模型**：悲观情景（下限）
- **P90模型**：乐观情景（上限）

### 6.3 不确定性量化

**目的**：评估预测的可靠性和风险

**方法**：分位数区间宽度

python

```python
# modeling.py
# 风险得分 = (上限 - 下限) / 中位数预测
risk_score = (predictions['P90'] - predictions['P10']) / np.maximum(
    predictions['P50'], 0.5
)
```

**作用**：风险得分高表示预测不确定性大，需要更谨慎决策