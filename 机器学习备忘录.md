# Chapter 1. 概述
## 1. 监督学习、无监督学习与强化学习
$$三种典型的机器学习：\begin{cases}
监督学习：标记数据\Rightarrow 直接反馈\Rightarrow 预测结果/未来\\
无监督学习：没有标签（目标）、没有反馈\Rightarrow 发现数据中的隐藏结构\\
强化学习：决策过程、奖励系统、学习一系列行为
\end{cases} $$

### 1.1 用监督学习对未来进行预测
监督学习的目标是**从标记好的训练数据中学习一个模型，使我们能够对未知或未来的数据进行预测**，这里”监督“指的是一组训练样例（数据输入），**这些输入的期望输出信号（标签）是已知的，==监督学习就是对数据输入与标签之间的关系进行建模的过程==（”标签学习“）**
典型的监督学习工作流程如下：
![[Pasted image 20240818210959.png]]
被赋予标签的训练数据集被传递给机器学习算法，拟合出一个预测模型，对新的未标记的数据输入进行标签的预测
**具有离散的类标签的监督学习任务称为分类任务；而输出信号（标签）为连续值的监督学习任务称为回归。**

### 1.1.1 预测类标签的分类任务
分类是监督学习的一个子类，**其目标是基于过去的观察，来预测新实例或数据点的分类类标签。** 这些类标签是**离散的、无序的值**，可以理解为**数据点的分组的组成员关系**
一个数据集可以有多个维度，代表每个样例有几个与之相关的值，例如：
二元分类任务：数据集是二维的，$x_1$和$x_2$是样例的两个维度
![[Pasted image 20240818212232.png]]
我们使用监督学习算法，来学习一条**规则**，即虚线表示的**决策边界**，它将数据分成A、B两组，并且当有新数据输入时将其按照$x_1$和$x_2$的值分进两种类别中的一个
类标签（组别数）不一定只有两个，通过监督学习算法学习到的预测模型可以把任何一个类标签分配给新的未标记的实例。

### 1.1.2 用于预测连续结果的回归
**第二类监督学习是对连续结果的预测，也称为==回归分析==。** 机器学习中，通常把解释变量称为**特征**，把响应变量称为**目标变量**。
最常见的线性回归就是：给定一些特征变量$\vec{X}$和目标变量$\vec{Y}$，在数据点中拟合出一条直线，使得数据点与拟合直线之间的距离（通常是平均平方距离）达到最小。
## ..............施工中

## 1.2 用强化学习解决交互问题
机器学习的另一种类型是**强化学习**，**强化学习的目标是开发一个基于与环境交互来提高其性能的系统（智能体）**，由于关于环境当前状态的信息还包括了所谓的 **==奖励信号==** ，我们可以认为强化学习和监督学习是相关的两个领域
不同的是，**强化学习中的反馈并不是基于确切的类标签或实值的，而是一种由==奖励函数==进行的对行动的衡量**
总之，**强化学习关注的是学习选择一系列使总奖励最大化的行动，这些行动既可以立马获得反馈，也可以延迟获得反馈**

## 1.3 用无监督学习发现隐藏结构
监督学习中，我们事先知道了答案（标签、目标变量），而强化学习中我们为智能体执行的行为定义了奖励度量。在无监督学习中，**我们处理的是未标记或结构未知的数据，在没有已知结果变量或奖励函数的情况下探索数据的结构来提取信息**

### 1.3.1 通过聚类分析找到子组
聚类是一种**探索性的数据分析或模式发现技术**，它允许我们在不事先知道其组成成员关系的情况下将一堆信息组织成有意义的子组

# Chapter 2. 训练简单的分类机器学习算法
本章我们将使用最早以算法描述的两种机器学习算法来进行分类任务：**感知器和自适应线性神经元**
## 2.1 M-P神经元模型
**人工神经元：具有二进制输出的简单逻辑门——多个信号到达树突，然后被整合到细胞体中，当累计的信号超过一定阈值，就会产生一个输出信号，由轴突传递**
### 2.1.1 人工神经元的正式定义
我们可以把人工神经元背后的思想放在一个**二元分类任务**下，该任务有两个类0和1
定义决策函数$\delta (z)$，**它接受特定的输入值$\vec{x}$以及相应的权重向量$\vec{\omega}$的线性组合，z是所谓的==净输入==：**$z = {\omega}_{1}x_{1} + {\omega}_{2}x_{2}+\cdots +{\omega}_{m}x_{m}$，$\vec{\omega}=[{\omega}_1,\cdots , {\omega}_m]^T$，$\vec{x}=[x_1,\cdots , x_m]^T$
现在，若一个实例$\vec{x}^{(i)}$的净输入大于定义的阈值$\theta$，我们预测其为类1；否则预测其为类0，即**决策函数为以下单位阶跃函数**：
$$ \delta (z) = \begin {cases}
1, \quad 如果z\geq \theta \\
0, \quad otherwise
\end {cases} $$
为了简化代码实现，我们对条件进行修改：
首先，将$z\geq \theta$修改为$z-\theta \geq 0$，定义**偏置单位**$b=-\theta$，使其成为净输入的一部分：
$$ z = {\omega}_1x_1 + {\omega}_2x_2 +\cdots + {\omega}_mx_m+b$$
然后重写决策函数为：
$$ \delta (z) = \begin {cases}
1, \quad 如果z\geq 0 \\
0, \quad otherwise
\end {cases} $$
这样$z=\vec{\omega}^T \vec{x} + b$就通过决策函数被压缩为**二进制输出**了，并且以$z\geq 0$作为决策边界分离出两种类别。

### 2.1.2 感知器学习规则
神经元与阈值感受器算法的思想是：**用简化的方式模拟大脑中单个神经元的工作方式**，因此步骤非常简单
==**感知器算法**==可以概括为以下步骤：
* **1. 将权重与偏差单位初始化为==0或较小的随机数==**
* **2. 对于每个训练样例$\vec{x}^{(i)}$，都做：
	* **a. 计算输出值$\hat{y}^{(i)}$
	* **b. 更新权重与偏差单位

这里输出值$\hat{y}^{(i)}$是我们之前定义的**单位阶跃函数所预测的类标签0或1**；
而更新偏置单位b与权重向量$\vec{\omega}$中的每个元素$\omega _j$的操作可以写为：
$$ \omega _j := \omega _ j + \Delta \omega_j \quad ;\quad b:= b+\Delta b $$
更新值$\Delta$的计算方法如下：
$$ \begin {align*}
\Delta \omega _j &= \eta (y^{(i)} - \hat{y}^{(i)})x_{j}^{(i)} \\
\Delta b &= \eta (y^{(i)} - \hat{y}^{(i)})x_{j}^{(i)}
\end {align*} $$
其中，**$\eta$是==学习率==，通常为0.0到1.0之间的常数；$\vec{\omega}$中的每一个权重$\omega _ j$都对应着数据集的一个特征$x_j$，它们会在每一步中被更新；$y^{(i)}$是第i个训练样本真实的类标签，而$\hat{y}^{(i)}$是模型预测得出的类标签。**
值得注意的是，**权重向量中的偏差单位和所有权重是==同时更新==的，因此在更新前我们无需重新计算$\hat{y}^{(i)}$。**
总之，我们把更新规则写为：
$$ \begin {align*}
\Delta \omega_1 &= \eta (y_{(i)}-输出^{(i)})x_1^{(i)} \\
\Delta \omega_2 &= \eta (y_{(i)}-输出^{(i)})x_2^{(i)} \\
\Delta b &= \eta (y^{(i)}-输出^{(i)})
\end {align*} $$
在进入正式的代码实现前，我们目测一下这个规则的简洁之美：
$$ \begin{cases}
当感知器成功预测时： y^{(i)}==\hat {y}^{(i)}\Rightarrow \Delta \omega_j= \Delta b=0\\
当感知器预测错误时，有：\begin {cases}
y^{(i)}=1,\hat{y}^{(i)}=0时：\Rightarrow \Delta \omega_j = \eta x_j^{(i)}, \Delta b=\eta \\
y^{(i)}=0,\hat{y}^{(i)}=1时：\Rightarrow \Delta \omega_j = -\eta x_j^{(i)}, \Delta b=-\eta \\
\end {cases} \\
\end {cases} $$

具体来说，假设$\eta = 1, x_j^{(i)}=1.5$，当我们将类0的$y^{(i)}$错误地估计为类1，导致$\Delta w_j + \Delta b = -2.5$，使得净输入$z=x_j^{(i)}*\omega_j + b$的值在下一次我们遇到这个实例时**更有可能低于阈值**，使得正确归类同一个例子为类0的概率更高。
我们还发现，**权重的更新值$\Delta w_j$与$x_j^{(i)}$的值成正比**，这意味着**我们会根据犯错的“离谱程度”相应地改变我们变动决策边界的力度**，以便正确分类

要注意，**感知器规则的收敛性只在两个类是==线性可分==的情况下被保证**；如果两个类不能被线性决策边界分开，那么**在未设置训练集迭代次数或可容忍错误阈值的情况下**，**权重永远不会停止更新**。关于非线性决策边界的分类算法会在后面讲到。
![[Pasted image 20240903184914.png]]
最后，在正式实现之前，我们用下图归纳感知器学习算法：
![[Pasted image 20240903185440.png]]

### 2.1.3 感知器学习算法的代码实现
本节我们使用的数据集为Chapter 1中提到的**鸢尾花数据集**，可以直接这么下载使用：
![[Pasted image 20240903185939.png]]
也可以输入网址，用pandas的read函数得到

我们将采取面向对象方法将感知器接口定义为Python中的类，它允许我们创建新的感知器对象，通过拟合的方法在数据中学习并单独地通过预测方法预测。作为惯例，**我们在命名时用下划线附加到那些不是在对象初始化时创建的，而是通过调用对象的其他方法或计算实现的属性**

#### 2.1.3.1 感知器类的实现

构造函数：
```
def __init__(self, eta = 0.01, n_iter = 50, random_state = 1):  
    self.eta = eta  
    self.n_iter = n_iter  
    self.random_state = random_state

''' 参数说明：
         eta:学习率，float类型；
      n_iter:迭代次数，可用于传递训练数据集，int类型
random_state:权重的随机数生成种子，int类型 '''
```

拟合训练数据集方法fit：
```
def fit(self, X, y):  
    rgen = np.random.RandomState(self.random_state)  
    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) 
    #生成一个符合0附近正态分布的随机数数组，赋值给权重数组w_；X的维度就是w的维度，维度个数就是数据集的特征个数 
    self.b_ = np.float_(0.)    #把偏置b设置为numpy内置的浮点数类型  
    self.errors_ = []   #列表，统计每次训练产生的预测错误次数  
    for _ in range(self.n_iter):  
        errors = 0  
        for xi, target in zip(X,y):   
        #zip函数用于把多个可迭代对象压缩在一起，生成一个由元组组成的迭代器，元素位置和声明时一样，可以*并行遍历*  
        #这里xi就是训练集的对象，target则是训练集的类标签  
            update = self.eta * (target - self.predict(xi))  #predict函数的实现在后面  
            self.w_ += update * xi  
            self.b_ += update   
            errors += int(update != 0.0)  #当每次发生更新时，说明预测出现了错误，递增errors  
        self.errors_.append(errors)   #把每次预测错误次数errors置入列表  
    return self
```
这里的X是我们传递的训练数据集中实例的特征数组，而y则是确定的类标签

计算净输入方法net_input：
```
def net_input(self, X):  
    return np.dot(X, self.w_) + self.b_
    #按公式计算即可，dot是numpy中的向量点积
```

有了净输入，就可以进行预测了，predict方法：
```
def predict(self, X):  
    return np.where(self.net_input(X) >= 0.0, 1, 0)  
    
    #where是np中的条件函数，np.where(condition, [x, y]) 代表若条件为True，结果为x；反之为y。这里根据净输入是否大于0分成类1和类0
```

整个感知器类是这样的：
![[Pasted image 20240903201548.png]]

使用上述感知器实现，我们现在可以使用一个给定的学习率eta，以及迭代次数n_iter（用于传递训练数据集）来初始化一个新的感知器对象了。
**通过fit方法，我们初始化偏置单位self.b_为0；初始化权重数组self.w_为一个$R^m$上的向量，其中m代表数据集的维度（特征）个数**
注意我们**并没有将权重向量的分量全部初始化为0**，这会导致**学习率eta对决策边界无法产生任何影响，只会影响权重向量的尺度而不改变其方向**，因此我们取权重向量各分量为**很小的随机数**

权重向量的方向可以通过使用numpy的反三角函数计算处角度来看出，不妨假设一个向量v1= \[1, 2, 3\]
```
v1 = np.array([1, 2, 3])
v2 = 0.5 * v1
np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
```
**这里的dot方法是向量点乘，而np.linal.norm()函数则用于计算向量的长度**
可以通过np.zeros(X.shape\[1])把w全部初始化为0，改变eta看看权重向量的方向是否改变（并不会改变）


> [!NOTE] numpy的向量化方法
> 前文的很多地方使用到了numpy库中的点积方法dot，它们也可以使用循环的方法计算，但是numpy这样**向量化的计算方法**的优势在于通过将算术运算表述为数组上的指令序列，而不是一次对每个元素执行一组操作，我们可以**更好地利用支持单指令多数据(SIMD)的现代中央处理单元(CPU)体系结构**；同时采用**高度优化的线性代数库**，允许我们直接使用线性代数的知识解决问题

#### 2.1.3.2 在鸢尾花数据集上训练感知器模型
为了测试我们的感知器实现，接下来我们将在鸢尾花数据集上仅仅考虑萼片长度和花瓣长度这两个维度的特征，训练我们的感知器模型，并且可视化决策边界。我们也只考虑二元分类情况，将鸢尾花分为setosa和versicolor两类，**感知器模型可以通过OvA（一对全）技术推广为类别大于2的多分类任务**

OvA技术简介：
OvA 技术将多类分类问题转换为多个二分类问题，步骤如下：  
1. 模型训练：
 * 对于每个类别 $C_i$，训练一个感知机模型，该模型将类别 $C_i$视为正类，所有其他类别视为负类。  
 * 这样，如果有 k 个类别，就会训练 k 个感知机模型。  
2. 预测阶段： 
* 对于一个新的样本，所有 k 个模型都会对其进行预测。  
* 选择**预测概率或分数最高的模型对应的类别**作为最终预测结果。

首先，我们使用pandas库来直接把鸢尾花数据集加载到一个DataFrame对象中，如下：
```
import os   #提供操作系统交互模块，支持目录操作、进程管理等行为，常用于编写自动化脚本和命令行工具  
import pandas as pd  
s = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"  
df = pd.read_csv(s, header = None, encoding = 'utf-8')  
print(df.tail())
```
这里将**数据集的后5行**打印出来，效果如下：
![[Pasted image 20240903213908.png]]
接下来，我们提取前100个实例，其中两种鸢尾花各有50朵，然后把类标签转换为1 (versicolor) 和 0 (setosa)，我们将其储存在向量y中，**它的pandas DataFrame中的value方法会产生相应的NumPy表示**
类似地，我们把这100个样本的第一特征列（花萼长度）和第三特征列（花瓣长度）提取出来，储存在特征矩阵X中，我们可以通过二维散点图将其可视化：

```python
import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
s = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"  
df = pd.read_csv(s, header = None, encoding = 'utf-8')  

y = df.iloc[0:100,4].values  
y = np.where(y == 'Iris-setosa',0, 1)  
#构造y向量:iloc索引器提取数据框中前100行，第5列的数据，用value方法转换为numpy数组  
#然后使用numpy的where函数把标签转换为0或1  
X = df.iloc[0:100, [0,2]].values  
#提取前100行，第1和3列的数据构造X矩阵并转换为numpy形式  
  
plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')  
#plt的scatter函数用于绘图，这里令X的前50个数据的第一个元素为横坐标，第二个元素为纵坐标，用红色圆圈'red','o'绘图，标签为'setosa'  
plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='s', label='versicolor')  
#绘制X的第50到100个元素，用蓝色方块'blue', 's'绘制，标签为'versicolor'  
  
plt.xlabel('sepal length (cm)')  
plt.ylabel("petal length (cm)")  
#设置x和y轴标签  

plt.legend(loc = 'upper left')  
#设置图例于左上方'upper left'
plt.show()  
#显示图形
```
这样生成的图形：
![[Pasted image 20240903221330.png]]
该图像显示了鸢尾花数据集中的实例沿着两个特征轴的分布，明显可见**线性决策边界可以将两个类别的鸢尾花分开**，适合使用感知器算法
现在，是时候进入正题，在该数据集上训练我们的感知器算法模型了。和前面一样，**我们在每次迭代中都会维护一个errors，用于记录误分类误差，以此检测算法是否收敛**：
```
import os   #提供操作系统交互模块，支持目录操作、进程管理等行为，常用于编写自动化脚本和命令行工具  
import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
from Perceptron import Perceptron  #把Perceptron.py放在项目目录下
  
s = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"  
df = pd.read_csv(s, header = None, encoding = 'utf-8')  
y = df.iloc[0:100,4].values  
y = np.where(y == 'Iris-setosa',0, 1)  
#构造y向量:iloc索引器提取数据框中前100行，第5列的数据，用value方法转换为numpy数组  
#然后使用numpy的where函数把标签转换为0或1  
X = df.iloc[0:100, [0,2]].values  
#提取前100行，第1和3列的数据构造X矩阵并转换为numpy形式  
  
ppn = Perceptron(eta = 0.1, n_iter = 10)  #创建感知器对象
ppn.fit(X, y)  #使用写好的fit方法
plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker = 'o')  #把维护的errors_数组描点画图，可视化为折线图
plt.xlabel('epochs')  
plt.ylabel('numbers of update')  
plt.show()
```
出来的更新次数-迭代次数图像如下：
![[Pasted image 20240905205530.png]]
可见更新次数在第6次迭代后平稳，**算法收敛**，接下来让我们编写**可视化边界的函数：**

```python
import os   #提供操作系统交互模块，支持目录操作、进程管理等行为，常用于编写自动化脚本和命令行工具  
import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
from matplotlib.colors import ListedColormap #用于管理颜色映射和绘制色图  
from Perceptron import Perceptron  
  
def plot_decision_regions(X, y, classifier, resolution = 0.02):  #classifier是我们想研究的分类器，resolution是步长（往后看）  
    markers = ('o', 's', '^', 'v', '<')  
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')  #这两个元组在后面绘制边界使用。函数里并未用到  
    cmap = ListedColormap(colors[:len(np.unique(y))])   #cmap是一个颜色映射对象，unique(y）得到不同类标签的数量，根据它来决定颜色  
  
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1  
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1  
    #设置范围：计算X的两个特征的最小值和最大值，适当拓宽1格边界使得图片能完全展现出来  
  
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))  
    #np.meshigrid方法生成二维网格数组，作为坐标矩阵，用于绘制决策边界，它接受两个一维数组，合成一个二维网格坐标数组(x1, x2)  
    #np.arrange方法生成范围内步长为resolution的数组，我们取步长为0.02（看参数的默认值）  
  
    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)  
    #np.array().T将网格坐标转换为二维数组，这里xx.ravel()用于把网格点展平为一维数组，方便形成二维数组供预测器使用  
    lab = lab.reshape(xx1.shape)  
    #将lab的形状调整为与xx1（也是xx2）相同  
  
    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)  
    #plt.contourf()方法绘制填充等高线图，前两个参数是网格坐标，第三个是网格点的类别，第四个alpha是透明度，cmap则是颜色映射  
    plt.xlim(xx1.min(), xx1.max())  
    plt.ylim(xx2.min(), xx2.max())  
    #设置坐标轴显示的范围  
  
    for idx, cl in enumerate(np.unique(y)):  
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx],  
                    marker=markers[idx], label=f'class {cl}',  
                    edgecolor='black')  
  
  
s = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"  
df = pd.read_csv(s, header = None, encoding = 'utf-8')  
y = df.iloc[0:100,4].values  
y = np.where(y == 'Iris-setosa',0, 1)  
#构造y向量:iloc索引器提取数据框中前100行，第5列的数据，用value方法转换为numpy数组  
#然后使用numpy的where函数把标签转换为0或1  
X = df.iloc[0:100, [0,2]].values  
#提取前100行，第1和3列的数据构造X矩阵并转换为numpy形式  
  
ppn = Perceptron(eta = 0.1, n_iter = 10)  
ppn.fit(X, y)  
  
#用训练好的模型ppn调用可视化函数  
plot_decision_regions(X, y, classifier=ppn)  
plt.xlabel('Sepal length [cm]')  
plt.ylabel('Petal length [cm]')  
plt.legend(loc='upper left')  
plt.show()
```
最后出来的效果是：
![[Pasted image 20240905221028.png]]
可见边界非常清晰。再次重申，感知器算法需要确保样本点能被**线性分离**，否则无法停止更新

## 2.2 自适应线性神经元和收敛性
本节将介绍另一种类型的**单层神经网络(NN)：自适应线性神经元Adaline**，它可以被认为是感知器算法的改进版本
Adaline算法说明和定义了**最小化连续损失函数**的概念，这也是后面其他用于分类的机器学习算法的基础
**Adaline规则（Widow-Hoff规则）** 与感知器规则的关键区别在于：**Adaline算法中，权重是基于==线性激活函数==进行更新的，而非单位阶跃函数**
在Adaline中，**线性激活函数$\delta (z)$就是净输入z的==恒等函数==：$\delta (z)=z$，而我们依然使用==阈值函数==来进行最终的预测。**
两者的流程图差别如下：
![[Pasted image 20240905222654.png]]
**Adaline算法将真实的类标签与线性激活函数的连续输出值进行比较，计算模型误差并更新权重**；而感知器算法直接用预测的类标与真实的类标签进行比较

### 2.2.1 用梯度下降最小化损失函数
监督式机器学习算法的关键要素之一是**在学习过程中要优化已定义的目标函数**，目标函数通常是我们希望最小化的**损失函数或成本函数**。
对于Adaline算法，我们定义损失函数$L$来学习模型的一个参数：**计算结果与真实类标签之间的==均方误差==$MSE$。**
$$ L(\vec w, b) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)}-\delta ({z^{(i)}}))^2$$
其中系数$\frac{1}{2}$只是为了方便推导出损失函数相对权重参数的**梯度**而加的常数。
与单位阶跃函数相比，线性连续激活函数$\delta (z)=z$的主要优点在于**损失函数变为一个可微函数，同时还是一个凸函数**，因此我们可以使用简单而又强大的**梯度下降优化算法**来找到最小化损失函数的那个权重，进行分类

如下图，梯度下降的主要思想是逐渐下降，**直到到达局部或全局损失最小值**。每次迭代中，在**梯度的相反方向**上迈出一步，**其步长由学习率和梯度的斜率决定**
![[Pasted image 20240908204522.png]]
利用梯度下降，我们采取与损失函数$L(\omega, b)$的梯度$\nabla L(\omega, b)$相反的方向来更新斜率：
$$ \begin {align}
\vec\omega&:=\vec\omega+\Delta \vec\omega，\quad b :=b+\Delta b\\
\Delta \vec \omega &= -\eta \nabla_{\vec\omega}L(\vec\omega , b),\quad \Delta b=-\eta \nabla_b L(\vec \omega, b)
\end {align}$$
这里把参数变化量定义为**负的梯度乘以学习率**
为了计算损失函数的梯度，我们计算**损失对偏差b的偏导数**：
$$ \frac{\partial L}{\partial b}=-\frac{2}{n}\sum_{i}(y^{(i)}-\delta(z^{(i)}))$$
而**损失对权重的单个分量的偏导数：**
$$ \frac{\partial L}{\partial \omega_j}=-\frac{2}{n}\sum_{i}(y^{(i)}-\delta(z^{(i)}))x_j^{(i)}$$
由于上述分子中的系数2仅仅是比例因子，我们可以通过调整学习率为原来的2倍来忽略掉该系数而不影响算法，那么更新规则写为：
$$ \begin  {align}
\Delta \omega_j &= -\eta \frac{\partial L}{\partial \omega_j}\\
\Delta b &= -\eta \frac{\partial L}{\partial b}
\end {align} $$
（均方损失函数MSE的偏导计算具体过程省略）

尽管Adaline学习规则看起来和感知器相差不大，但是我们要注意 **$\delta (z^{(i)}):z^{(i)}=\vec\omega^T\vec{x}^{(i)}+b$ 是一个实数类标签，而非整数类标签**。此外，**权重上升率是基于训练集中的所有样例计算的，而不是在每个样例过完之后单独更新**，因此这种方法又称为==**分批梯度下降**==。为与后面的概念避免混淆，我们把这个过程称为==**全批梯度下降**==

### 2.2.2 用python实现Adaline
由于Adaline相对感知器算法做出的改动主要在于拟合方法，我们只需要对这部分的代码进行改动，采取梯度下降方法最小化损失函数即可。
```python
class AdalineGD:
	def __init__(self, eta=0.01, n_iter=50, random_state=1):
		self.eta = eta
		self.n_iter = n_iter
		self.random_state = random_state

	def fit(self, X, y):
		rgen = np.random.RandomState(self.random_state)  #随机数种子
		self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1]) #初始化权重
		self.b_ = np.float_(0.)
		self.losses_ = []

		for i in range(self.n_iter):
			net_input = self.net_input(X)  #净输入
			output = self.activation(net_input)  #激活函数
			errors = (y - output)
			#下面都是通过梯度下降对权重和偏差进行更新
			self.w_ += self.eta * 2.0 * X.T.dot(errors) / X.shape[0]
			self.b_ += self.eta * 2.0 * errors.mean()
			loss = (error**2).mean()
			self.losses_.append(loss)
		return self

	def net_input(self. X):
		return np.dot(X, self.w_) + self.b_

	def activation(self, X):
		return X    #激活函数是线性的

	def predict(self, X):
		return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)
		#返回每一步预测的类标签
```
尽管规则似乎与感知器相同，但是要注意**梯度的计算是基于整个训练集的，而权重和偏差的更新又用到了梯度**，代码中`errors`是一个包含偏导数计算所需的$\sum_{i}(y^{(i)}-\delta(z)^{(i)})$中每一项的数值的数组，用于计算均方误差函数，权重则除去一个`X.shape[0]`即可
如果希望不使用`for`循环，降低算法中更新权重部分的时间复杂度，我们考虑使用向量化操作：在特征矩阵`X`与误差向量`errors`之间，使用矩阵-向量乘法
```python
self.w_ += self.eta * 2.* X.T.dot(errors) / X.shape[0]
```

最后，`self.losses_`这个列表用于收集损失值，检查算法是否收敛。上述的学习率`eta`和迭代数`n_iter`是算法的两个超参数，对其进行选择是调优模型的关键，在后面的章节会介绍超参数调优的技术与最佳实践，以让模型获得最大性能

再次注意，此处我们选择的激活函数$\delta (z)$的函数类型对代码本身不产生影响，这里是线性恒等函数，下一章则会采取非恒等、非线性的逻辑回归作为分类器的激活函数

### 2.2.3 特征缩放：标准化
很多机器学习算法都需要某种特征缩放以获取最佳性能（后面两章的主要内容），梯度下降就是受益算法之一
在梯度下降中，我们可以对每个特征利用其均值和方差进行标准化，使得其以0为中心，以1为方差
假设$x_j$是需要标准化的特征，下面就是标准化的公式：
$$ x_j^{'}=\frac{x_j-\mu_j}{\delta_j}$$
其中$\mu_j$和$\delta_j$是样本均值与标准差
标准化有利于梯度下降性能的原因之一是它将特征统一至相同的尺度，使得我们更容易找到一个适合所有权重与偏差的学习率，更新权重的效果更好，有利于稳定训练，经过更少的步骤达到全局损失最小

### 2.2.4 大规模机器学习与随机（在线）梯度下降SGD
在前面的算法中，梯度下降是对整体数据集的所有数据点进行计算的（全批梯度下降），当数据集的规模较大时，这样的方法代价非常高昂
随机梯度下降（在线梯度下降）SGD是一个近似的解决方案，它使用增量的方式更新每个训练样例的参数，而不是像之前那样使用整个数据集的数据（累计误差和）来进行权重的更新

SGD的思想是**使用小批量样本（甚至是一个样本）来近似整体数据的梯度，因而将“整个数据集计算的权重更新值”替换为“单一样本点计算的权重更新值”**，权重更新值也就是梯度乘上学习率：
$$ \Delta w_j = \frac{2\eta}{n}\sum_{i}(y^{(i)}-\delta(z^{(i)}))x_j^{(i)}\Rightarrow \Delta w_j = \eta (y^{(i)}-\delta(z^{(i)}))x_j^{(i)},\quad\quad \Delta b = \eta (y^{(i)}-\delta(z^{(i)}))$$
这样一来，处理每一个样本的时候就可以进行一次权重的更新，虽然只是梯度的近似值，但因为权重更新更加频繁，往往**更快到达收敛**；同时在**非线性损失函数**的情况下，SGD**更容易避开较浅的局部最小值**。
当然，这样的缺点在于每次更新具有噪声，并不是平稳下降的

SGD的处理方式使得它很适合在线学习：数据流式输入，无穷无尽，并且每接收到一个新的样本就需要进行更新，在每次更新完之后，样本就可以扔掉以优化存储空间
SGD的正确性在于**小批量数据计算得到的梯度近似值是整体数据梯度值的无偏估计**，因此虽然方向上会具有噪声，但是整体方向是正确的

SGD中，学习率也应该从固定的$\eta$值更改为能够自适应的，从而进一步接近真正的全局最小值：
$$ \eta \rightarrow \frac{C_1}{迭代次数+C_2}$$
其中C1和C2均为常数

#### 小批量梯度下降
SGD只使用一个样本来计算权重更新值，具有较大的噪声，作为一个折中策略，我们可以选择全量数据集中的部分数据，构成子集，通过这个子集计算梯度值以获取权重更新值，实现梯度下降，这样的好处是我们可以用向量化操作进一步减少计算的时间开销，以实现折中的效率较高且噪声较小的梯度下降

#### SGD代码
```python
coming soon
```

# Chapter 3. 使用Scikit-Learn库实现机器学习算法

本章会介绍更多的流行且强大的机器学习算法，这些算法在scikit-learn库中都具有实现，因此也会介绍其优缺点和实践应用

## 3.1 选择分类算法


# Chapter 6. 学习模型评估和超参数调优的最佳实践

# Chapter 7. 结合不同模型进行集成学习

第六章介绍了对单一模型的评估和调优，正如之前所述，不同的模型具有各自的适用场景与缺陷，在这一章中，我们将会基于单一模型，探索构建一组分类器集，使得具有比它的任何成员具有更优秀的预测性能的不同方法。
具体来说，本章介绍集成学习的目标包括：
- 基于多数投票进行预测
- 利用Bagging（随机采样）减少过拟合：通过从训练数据集中有放回地随机抽取样本，形成不同的数据组合
- 使用Boosting算法从弱学习器（性能仅略优于随机猜测的模型）中构建强大的模型，它能够从弱学习器的错误中不断学习

## 7.1 使用集成方法学习

集成方法的目标是将不同的分类器组合成一个元分类器，使其泛化性能优于单个分类器。
例如，多个专家模型针对同一问题提出了多个预测结果，集成方法允许我们使用一定的策略结合这些所有的预测结果以得到比单独预测更准确的结果

创建分类器的集成有多种方式，我们需要了解集成学习的基本原理，以及它们为什么通常被认为具有良好泛化性能
本章中，我们主要关注最流行的集成方法：使用多数投票原则
简单来说，多数投票意味着我们选择被多数专家分类器预测的那个类标签作为最终结果，通常是超过50%的支持率的那个结果（二分类），或是最高支持率的那个结果（多分类）

一个已经被我们知道的集成学习算法是随机森林，它集成了多个决策树分类器，而更普遍地，我们能够基于训练集，通过不同的分类算法训练出不同的分类器$(C_1,\dots,C_m)$，使用投票原则进行最终结果的集成
![[Pasted image 20250831220129.png]]

使用数学语言描述，使用相对或绝对多数投票原则，我们将每个单独的分类器$C_j$的预测结果使用众数函数$mode\{\}$结合，得到最终选择的类标签$\hat{y}$
$$ \hat{y}=mode\{C_1(x),C_2(x),\dots , C_m(x)\}$$

为了证明多数投票能够提升预测值的可信度，我们不妨使用组合数学的方式进行简单的证明
假设现在有n个基底分类器，且错误率均为$\varepsilon$，并且我们假设分类器之间相互独立且错误率互不相关，基于这样的假设，我们可以直接使用二项分布的分布函数得到错误的概率为
$$
P(y \geq k) = \sum_{k}^{n} \binom{n}{k} \, \varepsilon^{k} (1 - \varepsilon)^{n-k} = \varepsilon_{\text{ensemble}}
$$
此处$\binom{n}{k}=\frac{n!}{(n-k)!k!}$是组合数，求和是从k加到n。$\varepsilon_{\text{ensemble}}$就是集成体发生错误的概率
下面是绘制得到的基分类器与集成分类器错误的概率曲线，可以看到，只要基分类器的预测正确率大于随机猜测($\varepsilon > 0.5$)，就有集成分类器的正确率高于基分类器
![[Pasted image 20250905134320.png]]

## 7.2 使用多数投票结合分类器
下面我们不妨实现一个最简单的多数表决集成分类器算法，作为热身
我们的目标是构建一个更强大的，能够平衡每个单独的分类器在特定数据集上缺点的**元分类器**。使用数学语言描述，我们将带权投票以以下形式表述：
$$ \hat{y}=\arg \max \limits_i \sum\limits_{j=1}^{m}w_j\chi_A(C_j(x)=i)$$
其中，$w_j$是基分类器$C_j$被集成时所具有的权重；$\hat{y}$是集成体所预测的类标签；$A$是给定类标签的集合，而$\chi_A$是特征函数（指示函数），它只在满足$C_j(x)=i$的时候为1，否则为0。
