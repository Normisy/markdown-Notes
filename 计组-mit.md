# L1. 信息表示
## 1.1 信息论、不确定性、量化
使用$p_1, p_2, \cdots, p_N$表示当离散变量X取$x_1,x_2,\cdots,x_N$时的概率，对于更高的概率，其不确定性就越弱。信息就是用于确定变量取值是什么的，当变量是某个值（给定的事件）的概率越大，该事件的不确定性就越弱，因此不确定性与它的概率成反比。
在信息论中，信息量定义为：
$$ I(x_i)=\log_2(\frac{1}{p_i})$$
其单位是**位数bit**，二进制位中一个位只能取0或1，而不确定性则是对概率的倒数取以2为底的对数，这意味着**不确定性代表使用二进制对一个给定选择进行编码时，达到理论上最优编码方案所需要使用的位数（理想长度）**

这里“对事件编码时的理论最优编码方案”的含义是，**使用二进制位表示每个事件（取值）时，出现概率大的事件出现频率大，因此需要更少的二进制位以使得在大量数据存储下减少存储压力**

假设现在存在N个相等概率的事件，并且我们获得了一些数据，得知已经发生的事件类型在M个以内，那么这些数据的信息量就是
$$ I(data)=\log_2(\frac{1}{M\cdot (1/N)})=\log_2(\frac{N}{M}) \quad bits$$
值得注意的是，这样的计算公式会产生小数，而dui