# L1. 信息表示
## 1.1 信息论、不确定性、量化
使用$p_1, p_2, \cdots, p_N$表示当离散变量X取$x_1,x_2,\cdots,x_N$时的概率，对于更高的概率，其不确定性就越弱。信息就是用于确定变量取值是什么的，当变量是某个值（给定的事件）的概率越大，该事件的不确定性就越弱，因此不确定性与它的概率成反比。
在信息论中，信息量定义为：
$$ I(x_i)=\log_2(\frac{1}{p_i})$$
其单位是**位数bit**，二进制位中一个位只能取0或1，而不确定性则是对概率的倒数取以2为底的对数，这意味着**不确定性代表使用二进制对一个给定选择进行编码时，达到理论上最优编码方案所需要使用的位数（理想长度）**

这里“对事件编码时的理论最优编码方案”的含义是，**使用二进制位表示每个事件（取值）时，出现概率大的事件出现频率大，因此需要更少的二进制位以使得在大量数据存储下减少存储压力**

假设现在存在N个相等概率的事件，并且我们获得了一些数据，得知已经发生的事件类型在M个以内，那么这些数据的信息量就是
$$ I(data)=\log_2(\frac{1}{M\cdot (1/N)})=\log_2(\frac{N}{M}) \quad bits$$
值得注意的是，这样的计算公式会产生小数，而电路中只能处理二进制位，因此**真正的信息量计算中，对最终结果需要==向上取整==**

## 1.2 熵
信息量指导我们理想状态下应该为每个事件分配多少位，但它并不能评判一个编码策略的好坏。
为了评估我们在电路中使用的位编码策略的性能，信息论使用**随机变量的熵**H(X)来描述

**随机变量的熵**指的是**当学习随机变量的值时所接收到的期望信息量**，即：
$$H(X)=\text{E}(I(X))=\sum\limits_{i=1}^{N}p_i\cdot \log_2(\frac{1}{p})
$$
而对于一个编码策略来说，它的熵就是**该策略下的事件期望信息量**，即事件编码位数按照概率的加权平均数，一个好的策略的熵应该接近随机变量的熵

## 1.3 编码
接下来，我们就需要探究将数据编码为二进制位的策略。编码本质上是一组位字符串和一组对象之间的一一映射；这样的位表示可以是位数固定的，也可以是可变长度的
一个编码策略必须是一一对应的，并且在表示消息时不会出现二义性，即一串位只能被唯一地解码为一条信息

在图形上，我们可以将这样明确的编码使用二叉树表示，类似于词典树，两个子节点分别表示下一位取0和1，一条叶结点到根节点的路径代表一个字符的编码表示
![[Pasted image 20260206215534.png]]
叶结点到根节点的边数就是它的编码长度
而对于许多字符组成的字符串的二进制位表示，例如`01111`，则能够直接将这串位视为在二叉树中转向的指示，从根结点开始，遇到0向左子树转，遇到1向右子树转，遇到叶结点则回到根节点开始找下一个字符

### 1.3.1 固定长度编码
在我们认为一组符号的出现概率相同时，使用固定长度编码。在编码二叉树中，这意味着所有叶结点到根结点的边数相同，在数据序列中通过固定字长作为偏移量即可分离出每个符号
固定长度编码的熵值为
$$ \sum\limits_{i=1}^{N}(1/N)\log_2(\frac{1}{1/N})=\log_2 N$$
其中N为所有需要表示的字符的总量
编码策略的熵值也代表信息量的含义：每个字符需要熵值向上取整后得到的位数值来表示，即**固定字长等于熵值向上取整**

以常见的ASCII编码为例，它具有94个字符，至少需要$\log_2 (94)$的位来表示，该值向上取整为7位，熵值约为6.555，所以ASCII码长为7位

