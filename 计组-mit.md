# L1. 信息表示
## 1.1 信息论、不确定性、量化
使用$p_1, p_2, \cdots, p_N$表示当离散变量X取$x_1,x_2,\cdots,x_N$时的概率，对于更高的概率，其不确定性就越弱。信息就是用于确定变量取值是什么的，当变量是某个值（给定的事件）的概率越大，该事件的不确定性就越弱，因此不确定性与它的概率成反比。
在信息论中，信息量定义为：
$$ I(x_i)=\log_2(\frac{1}{p_i})$$
其单位是**位数bit**，二进制位中一个位只能取0或1，而不确定性则是对概率的倒数取以2为底的对数，这意味着**不确定性代表使用二进制对一个给定选择进行编码时，达到理论上最优编码方案所需要使用的位数（理想长度）**

这里“对事件编码时的理论最优编码方案”的含义是，**使用二进制位表示每个事件（取值）时，出现概率大的事件出现频率大，因此需要更少的二进制位以使得在大量数据存储下减少存储压力**

假设现在存在N个相等概率的事件，并且我们获得了一些数据，得知已经发生的事件类型在M个以内，那么这些数据的信息量就是
$$ I(data)=\log_2(\frac{1}{M\cdot (1/N)})=\log_2(\frac{N}{M}) \quad bits$$
值得注意的是，这样的计算公式会产生小数，而电路中只能处理二进制位，因此**真正的信息量计算中，对最终结果需要==向上取整==**

## 1.2 熵
信息量指导我们理想状态下应该为每个事件分配多少位，但它并不能评判一个编码策略的好坏。
为了评估我们在电路中使用的位编码策略的性能，信息论使用**随机变量的熵**H(X)来描述

**随机变量的熵**指的是**当学习随机变量的值时所接收到的期望信息量**，即：
$$H(X)=\text{E}(I(X))=\sum\limits_{i=1}^{N}p_i\cdot \log_2(\frac{1}{p})
$$
