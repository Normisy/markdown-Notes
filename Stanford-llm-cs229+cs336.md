# Lecture. CS229

- pretraining：预训练，在非常大规模的通用数据集上训练模型，使得模型学习到基本的语言模式、语义关系和世界知识
- post-training：后训练，llm预训练完成后，使用llm，对其进一步训练，使其更符合特定的用途和目标

回忆一下，高层次的语言模型实际上是tokens序列的概率分布，使用$p(x_1,\dots,x_L)$描述这样一个序列
语言模型是生成式模型：它能够生成tokens序列或数据，$x_{1:L}\sim p(x_1,\dots,x_L)$，这是因为一旦你具有一个这样的概率分布，就可以简单地从这个模型中进行采样以生成数据和句子

现在人们通常使用的语言模型是自回归语言模型：
$$ p(x_1, \dots, x_L)= p(x_1)p(x_2|x_1)p(x_3|x_2,x_1)\dots$$
即把这个概率分布拆解为每个给定前面单词存在的情况下，该单词存在的概率之积；这其中不存在近似值，只是条件概率的链式相乘而已，是一种对语言分布进行建模的方式
注意，这只是语言模型的其中一种建模方式，而不是唯一的方式，它具有它的优缺点，例如，其缺点之一在于生成长句子的时间开销很大，每个tokens都需要一次条件判断

高层次上来说，自回归语言模型的任务就是**预测：它预测下一个单词可能是什么**
因此，在语言模型的使用中，其步骤包括：
1. tokenize：将单词或子单词拆解为tokens，给予一个标记
2. 传递给下一个模型
3. 预测下一个token的概率
4. 采样
5. detokenize

下面是自回归语言模型在预测下一个token时所做的流程图：
1. embedding嵌入：计算机不能直接理解文字，它只能处理数字，因此在自然语言处理中，**需要将一个词语转变为连续空间中的向量，通过向量之间的关系捕捉语义关系**（Word2Vec，捕捉语义、转化为向量）
2. 神经网络处理上下文：所有词向量经过Transformer，通过注意力机制将词的信息捕捉起来，进一步捕获语义，输出一个维度等于隐藏层d的向量
3. 线性层映射：通过线性层将输出的d维向量h映射到整个词表|V|的维度
4. softmax生成概率分布（归一化）：softmax是一个函数，它接收一个向量，输出一个长度相同的向量，但其中的分量都在0-1之间且之和为1，即概率分布。这里将映射后维度与词表相同的向量归一化至概率分布
5. 选择概率最大的那个token输出
![[Pasted image 20250901193855.png]]

整个任务本质上**是对下一个token进行分类**（分类器）：给定一个上下文，选择最可能的下一个token，和经典的机器学习算法实际上是一脉相承的

模型训练过程中的示意图如下：
![[Pasted image 20250901204653.png]]

在训练过程中，我们为一个模型输入一句语料，例如`I saw a cat on a mat`，这句训练语料在`I saw a`的后面有且仅有一个正确的单词`cat`，因此使用单热编码，在`cat`对应的词表位置上标注1，其他均为0：$[0,\dots,1,\dots,0]$，这样的单热编码能够很简单地按照损失函数计算出损失，例如这里使用的是交叉熵损失。依据损失将每个位置的概率进行更新，例如这里就把除了`cat`以外的单词的概率下调，把`cat`的概率上调了
最后，在最简单的采样策略下，我们选择似然值最大的那个单词作为该上下文的下一个单词输出
$$ \max \prod_{i}p(x_i|x_{1:i-1})=\min(-\sum_i \log_2 p(x_i|x_{i:i-1}))=\min \mathcal{L}(x_{i:L})$$
$\mathcal{L}$就是损失函数

这里我们需要注意，**上面所提到的“每个单词的概率”完整含义应该是“==该上下文下==，每个单词出现在下一个token位置的概率**，在例子中也就是$p(* | I\quad saw\quad a)$这个条件概率
因此，大模型通过transformer拥有上下文建模能力，并将语义一并压缩至概率分布中，因此能够判断出下一个token应该是哪个（采样）
实际上，进行选择下一个token的采样时，不一定最好的采样就是似然最高的那个，可能出现多峰的情况，为了让结果更加丰富自然，会选取总概率到达一定阈值的一个子集，在其中随机采样

## 1.1 Tokenizer
可以看到，在语言模型中Tokenizer是非常重要的，因为实际上，我们遇到的情况更多：
- typo（拼写错误问题）：当一个单词出现了拼写错误，词表中可能没有任何与它相同的token，tokenizer需要对这种情况具有容错
- 其他语言：英文的tokenize很显然，按照空格或者词根词缀分离即可，但是我们需要对其他语言也进行tokenize，也转化为词向量。本质上说，这样tokenize出来的token，理论上是脱离所有语种的“世界语”
- 对于算法的时间复杂度而言，它随着序列的长度递增，因此tokenizer需要对一个词语的子部分进行进一步拆分，确保每个token尽量处于3-4个字母中

一个常见的用于tokenize的算法称为Byte Pair Encoding（BPE）：
1. 获取非常大量的语料文本
2. 首先将每个字母都对应上一个token![[Pasted image 20250902131946.png]]
3. 每次将最常见的一对tokens合并至一个token中![[Pasted image 20250902132032.png]]
4. 重复，直到达到所需的词汇大小或全部合并完成![[Pasted image 20250902132143.png]]

在一个非常大的文本语料库中进行训练，就能够对词句进行较为合理的tokenize了
并且可以发现，tokenize这一步是不区分一词多义的，字面上相同的词语会被赋予同一个tokenID，如前面所述，真正处理多义性的是上下文+transformer

实际上，tokenize是很多模型优化中重要的一环，因为这种方式决定了llm是如何看待文本的。例如对于数学中的数字，如果直接采用这样的tokenize方法就会导致很多数字（eg. 799）被单独归为一个token，大模型看待数学的方式就变得很诡异，因此之后的如gpt-4的大模型对这些特殊的文本作了特殊的tokenize策略

## 1.2 llm评估：困惑度perplexity

通常评估llm的方式是困惑度，概念上，它是一种对损失的验证，但是更具可解释性：
- 使用每个token的平均值，与词句长度独立
- 使用的损失函数$\mathcal{L}(x_{x:L})=-\sum\limits_{i=1}^{L}\log_2 p(x_i|x_{1:i-1})$是交叉熵损失，它是以2为底的平均对数损失，而PPL将其指数化是为了将其变回概率空间的度量
$$ PPL(x_{1:L})=2^{\frac{1}{L}\mathcal{L}(x_{1:L})}=\prod p(x_i|x_{1:i-1})^{-1/L}$$

PPL本质上是**所有条件概率的几何平均数的倒数**。反映了模型在预测每个token时平均分配给真实token的概率有多大（对预测token是正确的的信念有多大）
PPL的值域在1到词表大小之间，可以直观理解为“相当于在多少个token中随机挑选”，例如PPL=100相当于模型预测时相当于在100个token中随机挑选，这导致可信度不强

困惑度实际上主要和tokenizer有关，往往不用于学术上的评测，只是在自己训练模型时会检查

## 1.3 基准测试benchmark-NLP
另一个评估模型的方式是采取经典的NLP基准：收集大量可用于自动评估的基准测试指标，通过这些数据进行模型评估
最常见的两种基准测试是全局语言模型评估HELM和huggingface open LLM leaderboard，大部分通过向模型提问（选择题），然后观察它们的回答，收集正确的数据（MMLU）

## 1.4 缩放法则
经验上说，用于训练的数据越多，模型越大，似乎效果越好

# Chapter 1.  概览？
## 1.1 Basics（基础工作）
本单元的目标是实现基础版本的pipeline工作，包括tokenizer、模型架构和训练
1. Tokenizer：在字符串和整数序列之间进行转换的工作，负责将字符串分解成段并将每段映射到一个整数上，使其位于一个固定的维度上
2. 字节对编码BPE：本章将讨论的tokenizer算法是BPE，简单但仍在被使用。当然，现在也有论文探索Tokenizer-Free的方式，但是目前为止还没有到达边界

在将字符串转换为整数序列后，就需要为其指定一个模型架构。最原始的transformer架构由[Attention is All you need](https://arxiv.org/pdf/1706.03762)提出，在本章不会特别详细介绍，下面是其架构的示意图：
![[Pasted image 20250903204252.png]]
我们只需要注意到中间有个多头自注意力层和一个带有一些规范化的MLP层就好
大体来说，在transformer架构被发明出之后，人们使用的架构都是这一套，但是一直在对其进行部分改进以满足更高的性能要求，例如使用非线性激活函数、旋转位置嵌入、使用RMS范数进行标准化、full attention/sliding window attention/linear attention用于防止二次爆炸，以及甚至使用hyena状态空间取代attention机制（或是混合）等等

- optimizer（优化器）是大语言模型用于更新模型参数（权重和偏置）的算法，其作用在与根据损失函数的梯度来决定如何调整参数使得模型接近最优解。
	现在最常用的优化器是adam和其改进adamw，它们适合大规模的语言模型，也是课程作业中会重点关注的一环

- 学习率调度：训练过程中学习率随着训练动态变化，早期快速学习，后期精细调整以找到最优解

### system部分
系统部分主要关注的是如何通过发挥硬件的性能，以优化模型的构建和训练过程

#### 内核
一个GPU简单来说是由执行浮点运算的小单元组成的巨大计算阵列，并且GPU实际上是具有内存的，包括L1和L2缓存等
基本的思想是，最好将所有的计算拆分到GPU的计算单元上，而内存负责存储需要被计算的数据
在内存-GPU这样的关系下，我们可以发现一个性能瓶颈在于从内存将数据移动至gpu的这段传输路径上，很多技术可以优化这个环节的效率，例如后续会介绍的fusion（融合技术：将多个操作或计算合并为一个操作以减少中间数据存储运输的开销）和Tiling（分块技术：将大规模计算问题划分为更小块的计算，提高局部性以减少内存访问延迟、优化gpu利用率）

为了实现和利用内核，本课程后续将会介绍Triton,这是由OpenAI发布的内核构建工具，用户操作会更加“流行”
一般来说大型模型的训练需要使用到超过10000个gpu，但即使只有8个gpu，情况就已经变得复杂有趣了
![[Pasted image 20250918192016.png]]

我们发现，不仅仅是gpu会和cpu相连接，gpu之间也会通过NVSwitch、NVLink直接相互连接：gpu之间的数据传输会甚至更慢。因此我们需要考虑模型参数、梯度、激活函数等的计算应该尽量减少gpu之间的数据移动量，保持并行度（向量化技术等等）

#### 推理
单一大模型直接思考问题并回答的效果是很差的，需要让模型一步步进行推理，获得答案。但是和训练不同，推理的成本并不是一次性的：每次使用大模型都需要进行推理，因此其成本是会一直产生的，随着用户增多也会一直增加成本和效率需求

![[Pasted image 20250918200945.png]]
模型推理一共分为两个过程：
1. 预填充阶段（黄色背景部分）：接受Prompt输入，可以通过模型运行它并且获得一些激活函数的激活值，代表模型对prompt的初步理解，并将激活信息加载到KV缓存中，以保证模型对输入上下文的记忆
2. 解码阶段（绿色背景部分）：是一个迭代的过程——从第一个token被生成开始，就将其激活信息追加到KV缓存中，然后基于kv缓存存储的上下文继续生成下一个token,以此类推

这样的推理过程显然存在一个数据从缓存到模型之间频繁的传输，为了优化这个过程，需要在系统架构上做很多工作

缩放法则、数据管理、数据集、模型微调、强化学习等等，都作了简述，后续会详细介绍，不作记录

## 